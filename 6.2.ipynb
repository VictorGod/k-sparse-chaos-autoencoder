{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47c179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING ALL CRITICAL EXPERIMENTS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "K-SPARSE ABLATION STUDY\n",
      "================================================================================\n",
      "\n",
      "Testing K values: [4, 8, 16, 32, 64, 96, 112]\n",
      "Latent dimension: 128\n",
      "Sparsity range: 12.5% to 96.9%\n",
      "\n",
      "[K=4] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 3s 26ms/step - loss: 0.1361 - val_loss: 0.1352\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 0.1326 - val_loss: 0.1315\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 0.1296 - val_loss: 0.1280\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1272 - val_loss: 0.1254\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1251 - val_loss: 0.1238\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1235 - val_loss: 0.1222\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1220 - val_loss: 0.1210\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1210 - val_loss: 0.1203\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1201 - val_loss: 0.1194\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1194 - val_loss: 0.1190\n",
      "  Variance: 0.068083\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 96.9%\n",
      "\n",
      "[K=8] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 13ms/step - loss: 0.1360 - val_loss: 0.1335\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1327 - val_loss: 0.1302\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1296 - val_loss: 0.1277\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1272 - val_loss: 0.1250\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1252 - val_loss: 0.1236\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1234 - val_loss: 0.1224\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1220 - val_loss: 0.1210\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1210 - val_loss: 0.1202\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1200 - val_loss: 0.1196\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1194 - val_loss: 0.1188\n",
      "  Variance: 0.130408\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 93.8%\n",
      "\n",
      "[K=16] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 12ms/step - loss: 0.1362 - val_loss: 0.1329\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1328 - val_loss: 0.1301\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1298 - val_loss: 0.1276\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1273 - val_loss: 0.1250\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1253 - val_loss: 0.1238\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1236 - val_loss: 0.1223\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1222 - val_loss: 0.1213\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1211 - val_loss: 0.1202\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1204 - val_loss: 0.1197\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1196 - val_loss: 0.1192\n",
      "  Variance: 0.239458\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 87.5%\n",
      "\n",
      "[K=32] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 13ms/step - loss: 0.1371 - val_loss: 0.1339\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1337 - val_loss: 0.1303\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1307 - val_loss: 0.1279\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1282 - val_loss: 0.1261\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1262 - val_loss: 0.1245\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1244 - val_loss: 0.1232\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1232 - val_loss: 0.1220\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1221 - val_loss: 0.1212\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1212 - val_loss: 0.1203\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1205 - val_loss: 0.1199\n",
      "  Variance: 0.420444\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 75.0%\n",
      "\n",
      "[K=64] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 11ms/step - loss: 0.1388 - val_loss: 0.1352\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1351 - val_loss: 0.1318\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1321 - val_loss: 0.1290\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1295 - val_loss: 0.1275\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1275 - val_loss: 0.1256\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1258 - val_loss: 0.1242\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1244 - val_loss: 0.1231\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1234 - val_loss: 0.1224\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1225 - val_loss: 0.1218\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1218 - val_loss: 0.1212\n",
      "  Variance: 0.567432\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 50.0%\n",
      "\n",
      "[K=96] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 12ms/step - loss: 0.1395 - val_loss: 0.1357\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1358 - val_loss: 0.1326\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1328 - val_loss: 0.1301\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1301 - val_loss: 0.1281\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1281 - val_loss: 0.1262\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1264 - val_loss: 0.1252\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1251 - val_loss: 0.1237\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1240 - val_loss: 0.1228\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1231 - val_loss: 0.1225\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1224 - val_loss: 0.1219\n",
      "  Variance: 0.642433\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 25.0%\n",
      "\n",
      "[K=112] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 16ms/step - loss: 0.1396 - val_loss: 0.1359\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1360 - val_loss: 0.1329\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1329 - val_loss: 0.1300\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1305 - val_loss: 0.1277\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1283 - val_loss: 0.1266\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1266 - val_loss: 0.1250\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1253 - val_loss: 0.1240\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1242 - val_loss: 0.1231\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1234 - val_loss: 0.1225\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1226 - val_loss: 0.1220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_01_variance_vs_k (__main__.TestKSparseAblation)\n",
      "Test how variance changes with K ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variance: 0.657674\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 12.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_02_find_optimal_k (__main__.TestKSparseAblation)\n",
      "Find K that balances variance and sparsity ... ok\n",
      "test_03_create_ablation_plot (__main__.TestKSparseAblation)\n",
      "Create comprehensive ablation visualization ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST: Variance vs K\n",
      "================================================================================\n",
      "K=  4: variance = 0.068083\n",
      "K=  8: variance = 0.130408\n",
      "K= 16: variance = 0.239458\n",
      "K= 32: variance = 0.420444\n",
      "K= 64: variance = 0.567432\n",
      "K= 96: variance = 0.642433\n",
      "K=112: variance = 0.657674\n",
      "\n",
      "Trend violations: 0/6\n",
      "\n",
      "================================================================================\n",
      "TEST: Find Optimal K\n",
      "================================================================================\n",
      "K=  4: score = 1.651 (var=0.068, sparsity=96.9%)\n",
      "K=  8: score = 1.799 (var=0.130, sparsity=93.8%)\n",
      "K= 16: score = 1.774 (var=0.239, sparsity=87.5%)\n",
      "K= 32: score = 1.617 (var=0.420, sparsity=75.0%)\n",
      "K= 64: score = 1.113 (var=0.567, sparsity=50.0%)\n",
      "K= 96: score = 0.845 (var=0.642, sparsity=25.0%)\n",
      "K=112: score = 0.743 (var=0.658, sparsity=12.5%)\n",
      "\n",
      "‚≠ê Optimal K: 8\n",
      "   Variance: 0.130408\n",
      "   Sparsity: 93.8%\n",
      "\n",
      "================================================================================\n",
      "Creating K-Sparse Ablation Plots\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved: k_sparse_ablation.png\n",
      "\n",
      "================================================================================\n",
      "ABLATION RESULTS TABLE (LaTeX format)\n",
      "================================================================================\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\begin{tabular}{ccccc}\n",
      "\\hline\n",
      "K & Sparsity & Variance & Dead Neurons & Val Loss \\\\\n",
      "\\hline\n",
      "4 & 96.9% & 0.0681 & 0/128 & 0.1190 \\\\\n",
      "8 & 93.8% & 0.1304 & 0/128 & 0.1188 \\\\\n",
      "16 & 87.5% & 0.2395 & 0/128 & 0.1192 \\\\\n",
      "32 & 75.0% & 0.4204 & 0/128 & 0.1199 \\\\\n",
      "64 & 50.0% & 0.5674 & 0/128 & 0.1212 \\\\\n",
      "96 & 25.0% & 0.6424 & 0/128 & 0.1219 \\\\\n",
      "112 & 12.5% & 0.6577 & 0/128 & 0.1220 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{K-Sparse ablation study results}\n",
      "\\end{table}\n",
      "\n",
      "================================================================================\n",
      "MULTIPLE RUNS FOR STATISTICAL SIGNIFICANCE\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "Architecture: Dense_ReLU\n",
      "======================================================================\n",
      "\n",
      "[Run 1/5]\n",
      "  Variance: 0.063001\n",
      "  Dead neurons: 37/64\n",
      "  Val loss: 0.113788\n",
      "\n",
      "[Run 2/5]\n",
      "  Variance: 0.069018\n",
      "  Dead neurons: 27/64\n",
      "  Val loss: 0.113572\n",
      "\n",
      "[Run 3/5]\n",
      "  Variance: 0.098806\n",
      "  Dead neurons: 27/64\n",
      "  Val loss: 0.113449\n",
      "\n",
      "[Run 4/5]\n",
      "  Variance: 0.080280\n",
      "  Dead neurons: 31/64\n",
      "  Val loss: 0.113954\n",
      "\n",
      "[Run 5/5]\n",
      "  Variance: 0.064312\n",
      "  Dead neurons: 31/64\n",
      "  Val loss: 0.113693\n",
      "\n",
      "======================================================================\n",
      "Architecture: KSparse_Chaos\n",
      "======================================================================\n",
      "\n",
      "[Run 1/5]\n",
      "  Variance: 0.419252\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.119882\n",
      "\n",
      "[Run 2/5]\n",
      "  Variance: 0.420520\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.119782\n",
      "\n",
      "[Run 3/5]\n",
      "  Variance: 0.416230\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.119735\n",
      "\n",
      "[Run 4/5]\n",
      "  Variance: 0.419046\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.119661\n",
      "\n",
      "[Run 5/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_01_compute_statistics (__main__.TestMultipleRuns)\n",
      "Compute mean, std, confidence intervals ... ok\n",
      "test_02_statistical_comparison (__main__.TestMultipleRuns)\n",
      "Compare architectures with t-test ... ok\n",
      "test_03_create_errorbar_plot (__main__.TestMultipleRuns)\n",
      "Create plot with error bars ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variance: 0.420264\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.119950\n",
      "\n",
      "================================================================================\n",
      "TEST: Statistical Summary\n",
      "================================================================================\n",
      "\n",
      "Dense_ReLU:\n",
      "==================================================\n",
      "Variance:     0.075083 ¬± 0.013331 (CI: ¬±0.011685)\n",
      "Dead neurons: 30.6 ¬± 3.7\n",
      "Val loss:     0.113691 ¬± 0.000174\n",
      "\n",
      "KSparse_Chaos:\n",
      "==================================================\n",
      "Variance:     0.419062 ¬± 0.001525 (CI: ¬±0.001336)\n",
      "Dead neurons: 0.0 ¬± 0.0\n",
      "Val loss:     0.119802 ¬± 0.000103\n",
      "\n",
      "================================================================================\n",
      "TEST: Statistical Comparison (t-test)\n",
      "================================================================================\n",
      "\n",
      "Comparing Dense_ReLU vs KSparse_Chaos:\n",
      "  t-statistic: -51.2722\n",
      "  p-value: 0.000000\n",
      "  ‚úì SIGNIFICANT difference (p < 0.05)\n",
      "  ‚Üí KSparse_Chaos has significantly higher variance\n",
      "\n",
      "================================================================================\n",
      "Creating Error Bar Plots\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved: multiple_runs_comparison.png\n",
      "\n",
      "================================================================================\n",
      "HENON MAP GENERALIZATION TEST\n",
      "================================================================================\n",
      "Logistic train: (2000, 28, 28, 1)\n",
      "Henon train: (2000, 28, 28, 1)\n",
      "\n",
      "======================================================================\n",
      "Training K-Sparse Chaos on Logistic Map\n",
      "======================================================================\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 15ms/step - loss: 0.1368 - val_loss: 0.1339\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1336 - val_loss: 0.1309\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1306 - val_loss: 0.1284\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1282 - val_loss: 0.1263\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1260 - val_loss: 0.1247\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1245 - val_loss: 0.1231\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1231 - val_loss: 0.1222\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1219 - val_loss: 0.1214\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.1210 - val_loss: 0.1208\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.1203 - val_loss: 0.1200\n",
      "\n",
      "Results:\n",
      "  Variance: 0.421314\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.120015\n",
      "\n",
      "======================================================================\n",
      "Training K-Sparse Chaos on Henon Map\n",
      "======================================================================\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 17ms/step - loss: 0.0769 - val_loss: 0.0744\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0680 - val_loss: 0.0650\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0600 - val_loss: 0.0563\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0535\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0554 - val_loss: 0.0495\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0530 - val_loss: 0.0522\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0527 - val_loss: 0.0463\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.0498 - val_loss: 0.0482\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.0476 - val_loss: 0.0464\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0485 - val_loss: 0.0460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_01_compare_datasets (__main__.TestHenonGeneralization)\n",
      "Compare results across datasets ... ok\n",
      "test_02_visualize_latent_spaces (__main__.TestHenonGeneralization)\n",
      "Visualize and compare latent spaces ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Variance: 0.421981\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.045990\n",
      "\n",
      "================================================================================\n",
      "TEST: Logistic vs Henon Comparison\n",
      "================================================================================\n",
      "\n",
      "Metric                    Logistic        Henon           Ratio\n",
      "------------------------------------------------------------\n",
      "Variance                  0.421314        0.421981        1.00√ó\n",
      "Dead Neurons              0               0               -\n",
      "Val Loss                  0.120015        0.045990        0.38√ó\n",
      "\n",
      "================================================================================\n",
      "Visualizing Latent Spaces\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_01_track_broken_ae_death (__main__.TestDeadNeuronTrajectory)\n",
      "Track neuron death in broken L1 architecture ... skipped 'Requires build_sparse_ae_broken function'\n",
      "test_02_track_chaos_ae_stability (__main__.TestDeadNeuronTrajectory)\n",
      "Verify neurons stay alive in Chaos AE ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved: henon_generalization.png\n",
      "\n",
      "================================================================================\n",
      "TEST: Tracking Neuron Death (Broken L1 AE)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TEST: Neuron Stability (K-Sparse Chaos AE)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 10 tests in 159.266s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved: training_stability.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT SUMMARY\n",
      "================================================================================\n",
      "Total tests run: 10\n",
      "Successes: 10\n",
      "Failures: 0\n",
      "Errors: 0\n",
      "\n",
      "‚úÖ ALL CRITICAL EXPERIMENTS PASSED!\n",
      "\n",
      "Generated files:\n",
      "  - k_sparse_ablation.png\n",
      "  - multiple_runs_comparison.png\n",
      "  - henon_generalization.png\n",
      "  - training_stability.png\n",
      "\n",
      "Ready for paper! üéâ\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "CRITICAL EXPERIMENTS FOR PAPER\n",
    "=============================================================================\n",
    "Priority experiments:\n",
    "1. K-Sparse Ablation Study (different K values)\n",
    "2. Multiple Runs (statistical significance)\n",
    "3. Henon Map Dataset (generalization)\n",
    "4. Activation Distribution Analysis\n",
    "5. Training Curve Comparison\n",
    "6. Dead Neuron Trajectory Tracking\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import unittest\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Import your architectures\n",
    "# from sparse_ae_evolution import (\n",
    "#     build_dense_autoencoder,\n",
    "#     build_sparse_ae_broken,\n",
    "#     build_sparse_ae_simple_fix,\n",
    "#     build_sparse_ae_ksparse_relu,\n",
    "#     build_sparse_ae_ksparse_chaos,\n",
    "# )\n",
    "\n",
    "############################################\n",
    "# DATA GENERATORS\n",
    "############################################\n",
    "\n",
    "def logistic_map(x, r=3.99):\n",
    "    return r * x * (1 - x)\n",
    "\n",
    "def generate_logistic_map_image(image_size=28, initial_value=0.4, r=3.99):\n",
    "    iterations = image_size * image_size\n",
    "    x = initial_value\n",
    "    seq = []\n",
    "    for _ in range(iterations):\n",
    "        x = logistic_map(x, r)\n",
    "        seq.append(x)\n",
    "    img = np.array(seq).reshape((image_size, image_size))\n",
    "    return img\n",
    "\n",
    "def generate_logistic_dataset(num_images, image_size=28, r=3.99, fixed_initial=False):\n",
    "    dataset = []\n",
    "    for _ in range(num_images):\n",
    "        init_val = 0.4 if fixed_initial else np.random.rand()\n",
    "        img = generate_logistic_map_image(image_size=image_size, initial_value=init_val, r=r)\n",
    "        dataset.append(img)\n",
    "    return np.array(dataset)[..., np.newaxis].astype('float32')\n",
    "\n",
    "def henon_map(x, y, a=1.4, b=0.3):\n",
    "    \"\"\"Henon attractor - 2D chaotic system\"\"\"\n",
    "    x = np.clip(x, -2.0, 2.0)\n",
    "    y = np.clip(y, -2.0, 2.0)\n",
    "    \n",
    "    x_new = 1 - a * x**2 + y\n",
    "    y_new = b * x\n",
    "\n",
    "    return x_new, y_new\n",
    "\n",
    "def generate_henon_image(image_size=28, x0=0.1, y0=0.1, a=1.4, b=0.3):\n",
    "    \"\"\"Generate image from Henon attractor trajectory\"\"\"\n",
    "    iterations = image_size * image_size\n",
    "    x, y = x0, y0\n",
    "    points = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        x, y = henon_map(x, y, a, b)\n",
    "        # Normalize to [0, 1] (Henon values roughly in [-1.5, 1.5])\n",
    "        normalized = (x + 2.0) / 4.0\n",
    "        points.append(np.clip(normalized, 0, 1))\n",
    "\n",
    "    img = np.array(points).reshape((image_size, image_size))\n",
    "    return img\n",
    "\n",
    "def generate_henon_dataset(num_images, image_size=28, a=1.4, b=0.3):\n",
    "    dataset = []\n",
    "    for _ in range(num_images):\n",
    "        x0 = np.random.uniform(-0.5, 0.5)\n",
    "        y0 = np.random.uniform(-0.5, 0.5)\n",
    "        img = generate_henon_image(image_size, x0, y0, a, b)\n",
    "        dataset.append(img)\n",
    "    return np.array(dataset)[..., np.newaxis].astype('float32')\n",
    "\n",
    "\n",
    "class KSparseLayer(layers.Layer):\n",
    "    def __init__(self, k=32, **kwargs):\n",
    "        super(KSparseLayer, self).__init__(**kwargs)\n",
    "        self.k = k\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        latent_dim = tf.shape(inputs)[1]\n",
    "        values, indices = tf.nn.top_k(tf.abs(inputs), k=self.k, sorted=False)\n",
    "        mask = tf.reduce_sum(\n",
    "            tf.one_hot(indices, latent_dim, dtype=inputs.dtype),\n",
    "            axis=1\n",
    "        )\n",
    "        return inputs * mask\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"k\": self.k})\n",
    "        return config\n",
    "\n",
    "class TargetVarianceRegularizer(layers.Layer):\n",
    "    def __init__(self, lambda_reg=0.01, target_variance=0.1, **kwargs):\n",
    "        super(TargetVarianceRegularizer, self).__init__(**kwargs)\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.target_variance = target_variance\n",
    "\n",
    "    def call(self, inputs):\n",
    "        current_variance = tf.math.reduce_variance(inputs, axis=0)\n",
    "        mean_variance = tf.reduce_mean(current_variance)\n",
    "        variance_penalty = self.lambda_reg * tf.square(\n",
    "            mean_variance - self.target_variance\n",
    "        )\n",
    "        self.add_loss(variance_penalty)\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"lambda_reg\": self.lambda_reg,\n",
    "            \"target_variance\": self.target_variance\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def chaos_activation(x):\n",
    "    return tf.sin(8.0 * x) + 0.5 * tf.tanh(4.0 * x)\n",
    "\n",
    "\n",
    "def build_ksparse_chaos_ae(image_size=(28, 28), latent_dim=128, k_active=32):\n",
    "    \"\"\"K-Sparse Chaos autoencoder\"\"\"\n",
    "    input_img = keras.Input(shape=(*image_size, 1))\n",
    "    x = layers.Flatten()(input_img)\n",
    "\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.Activation(chaos_activation)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    latent_pre = layers.Dense(latent_dim, name='latent_pre')(x)\n",
    "    latent_pre = layers.Activation(chaos_activation)(latent_pre)\n",
    "\n",
    "    latent = KSparseLayer(k=k_active, name='latent_ksparse')(latent_pre)\n",
    "    latent = TargetVarianceRegularizer(\n",
    "        lambda_reg=0.01,\n",
    "        target_variance=0.1\n",
    "    )(latent)\n",
    "\n",
    "    encoder = keras.Model(input_img, latent, name='ksparse_chaos_encoder')\n",
    "\n",
    "    x = layers.Dense(256)(latent)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(chaos_activation)(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    decoded = layers.Dense(np.prod(image_size), activation='sigmoid')(x)\n",
    "    decoded = layers.Reshape((*image_size, 1))(decoded)\n",
    "\n",
    "    autoencoder = keras.Model(input_img, decoded, name='ksparse_chaos_autoencoder')\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def build_dense_relu_ae(image_size=(28, 28), latent_dim=64):\n",
    "    \"\"\"Dense ReLU baseline\"\"\"\n",
    "    h, w = image_size\n",
    "    input_img = keras.Input(shape=(h, w, 1))\n",
    "    x = layers.Flatten()(input_img)\n",
    "\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    latent = layers.Dense(latent_dim, activation=\"relu\", name=\"latent\")(x)\n",
    "\n",
    "    encoder = keras.Model(input_img, latent, name=\"dense_encoder\")\n",
    "\n",
    "    x = layers.Dense(128, activation=\"relu\")(latent)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dense(h * w, activation=\"sigmoid\")(x)\n",
    "    decoded = layers.Reshape((h, w, 1))(x)\n",
    "\n",
    "    autoencoder = keras.Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "\n",
    "\n",
    "def analyze_latent_statistics(encoder, images, zero_threshold=1e-6):\n",
    "    \"\"\"Comprehensive latent space analysis\"\"\"\n",
    "    latents = encoder.predict(images, verbose=0)\n",
    "\n",
    "    # Variance\n",
    "    variance_per_dim = np.var(latents, axis=0)\n",
    "    mean_variance = float(np.mean(variance_per_dim))\n",
    "\n",
    "    # Dead neurons\n",
    "    dead_mask = np.all(np.abs(latents) < zero_threshold, axis=0)\n",
    "    dead_neurons = int(np.sum(dead_mask))\n",
    "    total_neurons = latents.shape[1]\n",
    "\n",
    "    # Sparsity\n",
    "    zero_mask = np.abs(latents) < zero_threshold\n",
    "    overall_sparsity = np.mean(zero_mask)\n",
    "\n",
    "    # Active neurons statistics\n",
    "    active_per_sample = np.sum(~zero_mask, axis=1)\n",
    "    mean_active = np.mean(active_per_sample)\n",
    "\n",
    "    # Variance of active neurons only\n",
    "    variance_active = []\n",
    "    for dim in range(latents.shape[1]):\n",
    "        dim_values = latents[:, dim]\n",
    "        active_mask = ~zero_mask[:, dim]\n",
    "        if np.sum(active_mask) > 1:\n",
    "            active_values = dim_values[active_mask]\n",
    "            variance_active.append(np.var(active_values))\n",
    "\n",
    "    mean_variance_active = np.mean(variance_active) if variance_active else 0.0\n",
    "\n",
    "    return {\n",
    "        'variance_per_dim': variance_per_dim,\n",
    "        'mean_variance': mean_variance,\n",
    "        'dead_neurons': dead_neurons,\n",
    "        'total_neurons': total_neurons,\n",
    "        'dead_percentage': dead_neurons / total_neurons,\n",
    "        'overall_sparsity': overall_sparsity,\n",
    "        'mean_active_neurons': mean_active,\n",
    "        'mean_variance_active': mean_variance_active,\n",
    "        'latents': latents\n",
    "    }\n",
    "\n",
    "def track_dead_neurons_over_time(model, encoder, images, epochs=50, batch_size=64):\n",
    "    \"\"\"Track how neurons die during training\"\"\"\n",
    "    trajectory = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train one epoch\n",
    "        model.fit(images, images, epochs=1, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Analyze current state\n",
    "        stats = analyze_latent_statistics(encoder, images[:200])\n",
    "\n",
    "        trajectory.append({\n",
    "            'epoch': epoch,\n",
    "            'dead_neurons': stats['dead_neurons'],\n",
    "            'mean_variance': stats['mean_variance'],\n",
    "            'sparsity': stats['overall_sparsity']\n",
    "        })\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "class TestKSparseAblation(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    CRITICAL EXPERIMENT #1: K-Sparse Ablation Study\n",
    "    Test different K values to find optimal sparsity\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"K-SPARSE ABLATION STUDY\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Generate data once\n",
    "        cls.train_images = generate_logistic_dataset(2000, fixed_initial=False)\n",
    "        cls.test_images = generate_logistic_dataset(500, fixed_initial=False)\n",
    "\n",
    "        # Test different K values\n",
    "        cls.k_values = [4, 8, 16, 32, 64, 96, 112]\n",
    "        cls.latent_dim = 128\n",
    "        cls.results = {}\n",
    "\n",
    "        print(f\"\\nTesting K values: {cls.k_values}\")\n",
    "        print(f\"Latent dimension: {cls.latent_dim}\")\n",
    "        print(f\"Sparsity range: {(cls.latent_dim - max(cls.k_values))/cls.latent_dim:.1%} to {(cls.latent_dim - min(cls.k_values))/cls.latent_dim:.1%}\")\n",
    "\n",
    "        # Train models for each K\n",
    "        for k in cls.k_values:\n",
    "            print(f\"\\n[K={k}] Training K-Sparse Chaos AE...\")\n",
    "            ae, enc = build_ksparse_chaos_ae(\n",
    "                latent_dim=cls.latent_dim,\n",
    "                k_active=k\n",
    "            )\n",
    "\n",
    "            history = ae.fit(\n",
    "                cls.train_images, cls.train_images,\n",
    "                epochs=10,\n",
    "                batch_size=64,\n",
    "                validation_split=0.1,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Analyze\n",
    "            stats = analyze_latent_statistics(enc, cls.test_images)\n",
    "\n",
    "            cls.results[k] = {\n",
    "                'autoencoder': ae,\n",
    "                'encoder': enc,\n",
    "                'stats': stats,\n",
    "                'val_loss': history.history['val_loss'][-1],\n",
    "                'sparsity': (cls.latent_dim - k) / cls.latent_dim\n",
    "            }\n",
    "\n",
    "            print(f\"  Variance: {stats['mean_variance']:.6f}\")\n",
    "            print(f\"  Dead neurons: {stats['dead_neurons']}/{cls.latent_dim}\")\n",
    "            print(f\"  Sparsity: {cls.results[k]['sparsity']:.1%}\")\n",
    "\n",
    "    def test_01_variance_vs_k(self):\n",
    "        \"\"\"Test how variance changes with K\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Variance vs K\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        for k in self.k_values:\n",
    "            variance = self.results[k]['stats']['mean_variance']\n",
    "            print(f\"K={k:3d}: variance = {variance:.6f}\")\n",
    "\n",
    "        # Check that variance generally increases with K\n",
    "        variances = [self.results[k]['stats']['mean_variance'] for k in self.k_values]\n",
    "\n",
    "        # At least monotonic trend (allowing some noise)\n",
    "        trend_violations = 0\n",
    "        for i in range(len(variances)-1):\n",
    "            if variances[i+1] < variances[i]:\n",
    "                trend_violations += 1\n",
    "\n",
    "        print(f\"\\nTrend violations: {trend_violations}/{len(variances)-1}\")\n",
    "        self.assertLess(trend_violations, len(variances)//2,\n",
    "                       \"Variance should generally increase with K\")\n",
    "\n",
    "    def test_02_find_optimal_k(self):\n",
    "        \"\"\"Find K that balances variance and sparsity\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Find Optimal K\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Define metric: variance / (1 - sparsity)\n",
    "        # Higher = better efficiency\n",
    "        scores = {}\n",
    "        for k in self.k_values:\n",
    "            variance = self.results[k]['stats']['mean_variance']\n",
    "            sparsity = self.results[k]['sparsity']\n",
    "            score = variance / (1 - sparsity + 0.01)  # avoid division by zero\n",
    "            scores[k] = score\n",
    "            print(f\"K={k:3d}: score = {score:.3f} (var={variance:.3f}, sparsity={sparsity:.1%})\")\n",
    "\n",
    "        optimal_k = max(scores, key=scores.get)\n",
    "        print(f\"\\n‚≠ê Optimal K: {optimal_k}\")\n",
    "        print(f\"   Variance: {self.results[optimal_k]['stats']['mean_variance']:.6f}\")\n",
    "        print(f\"   Sparsity: {self.results[optimal_k]['sparsity']:.1%}\")\n",
    "\n",
    "        # Save result\n",
    "        self.optimal_k = optimal_k\n",
    "\n",
    "    def test_03_create_ablation_plot(self):\n",
    "        \"\"\"Create comprehensive ablation visualization\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Creating K-Sparse Ablation Plots\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "        k_vals = self.k_values\n",
    "        variances = [self.results[k]['stats']['mean_variance'] for k in k_vals]\n",
    "        sparsities = [self.results[k]['sparsity'] for k in k_vals]\n",
    "        dead = [self.results[k]['stats']['dead_neurons'] for k in k_vals]\n",
    "        losses = [self.results[k]['val_loss'] for k in k_vals]\n",
    "\n",
    "        # Plot 1: Variance vs K\n",
    "        axes[0, 0].plot(k_vals, variances, 'o-', linewidth=2, markersize=8)\n",
    "        axes[0, 0].set_xlabel('K (Active Neurons)')\n",
    "        axes[0, 0].set_ylabel('Mean Variance')\n",
    "        axes[0, 0].set_title('Variance vs K', fontweight='bold')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].axvline(x=32, color='red', linestyle='--', alpha=0.5, label='K=32 (your choice)')\n",
    "        axes[0, 0].legend()\n",
    "\n",
    "        # Plot 2: Sparsity vs Variance\n",
    "        axes[0, 1].plot(sparsities, variances, 'o-', linewidth=2, markersize=8)\n",
    "        axes[0, 1].set_xlabel('Sparsity')\n",
    "        axes[0, 1].set_ylabel('Mean Variance')\n",
    "        axes[0, 1].set_title('Sparsity-Variance Trade-off', fontweight='bold')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Annotate K=32\n",
    "        idx_32 = k_vals.index(32)\n",
    "        axes[0, 1].annotate('K=32',\n",
    "                           xy=(sparsities[idx_32], variances[idx_32]),\n",
    "                           xytext=(10, 10), textcoords='offset points',\n",
    "                           bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "                           arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "        # Plot 3: Dead Neurons vs K\n",
    "        axes[1, 0].bar(range(len(k_vals)), dead, color='steelblue', alpha=0.7)\n",
    "        axes[1, 0].set_xticks(range(len(k_vals)))\n",
    "        axes[1, 0].set_xticklabels(k_vals)\n",
    "        axes[1, 0].set_xlabel('K (Active Neurons)')\n",
    "        axes[1, 0].set_ylabel('Dead Neurons')\n",
    "        axes[1, 0].set_title('Dead Neurons vs K', fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # Plot 4: Reconstruction Loss vs K\n",
    "        axes[1, 1].plot(k_vals, losses, 'o-', linewidth=2, markersize=8, color='green')\n",
    "        axes[1, 1].set_xlabel('K (Active Neurons)')\n",
    "        axes[1, 1].set_ylabel('Validation Loss (MSE)')\n",
    "        axes[1, 1].set_title('Reconstruction Quality vs K', fontweight='bold')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.suptitle('K-Sparse Ablation Study: Finding Optimal Sparsity',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('k_sparse_ablation.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: k_sparse_ablation.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Also create a summary table\n",
    "        self.create_ablation_table()\n",
    "\n",
    "    def create_ablation_table(self):\n",
    "        \"\"\"Create LaTeX-ready table\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ABLATION RESULTS TABLE (LaTeX format)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        print(\"\\\\begin{table}[h]\")\n",
    "        print(\"\\\\centering\")\n",
    "        print(\"\\\\begin{tabular}{ccccc}\")\n",
    "        print(\"\\\\hline\")\n",
    "        print(\"K & Sparsity & Variance & Dead Neurons & Val Loss \\\\\\\\\")\n",
    "        print(\"\\\\hline\")\n",
    "\n",
    "        for k in self.k_values:\n",
    "            r = self.results[k]\n",
    "            print(f\"{k} & {r['sparsity']:.1%} & {r['stats']['mean_variance']:.4f} & \"\n",
    "                  f\"{r['stats']['dead_neurons']}/{self.latent_dim} & {r['val_loss']:.4f} \\\\\\\\\")\n",
    "\n",
    "        print(\"\\\\hline\")\n",
    "        print(\"\\\\end{tabular}\")\n",
    "        print(\"\\\\caption{K-Sparse ablation study results}\")\n",
    "        print(\"\\\\end{table}\")\n",
    "\n",
    "\n",
    "class TestMultipleRuns(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    CRITICAL EXPERIMENT #2: Multiple Runs for Statistical Significance\n",
    "    Run each architecture 5-10 times with different seeds\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MULTIPLE RUNS FOR STATISTICAL SIGNIFICANCE\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        cls.num_runs = 5  # –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–æ 10\n",
    "        cls.architectures = {\n",
    "            'Dense_ReLU': lambda: build_dense_relu_ae(latent_dim=64),\n",
    "            'KSparse_Chaos': lambda: build_ksparse_chaos_ae(latent_dim=128, k_active=32)\n",
    "        }\n",
    "\n",
    "        cls.results = {name: [] for name in cls.architectures.keys()}\n",
    "\n",
    "        # Generate data once\n",
    "        cls.train_images = generate_logistic_dataset(2000, fixed_initial=False)\n",
    "        cls.test_images = generate_logistic_dataset(500, fixed_initial=False)\n",
    "\n",
    "        # Run experiments\n",
    "        for arch_name, builder in cls.architectures.items():\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Architecture: {arch_name}\")\n",
    "            print(f\"{'='*70}\")\n",
    "\n",
    "            for run in range(cls.num_runs):\n",
    "                print(f\"\\n[Run {run+1}/{cls.num_runs}]\")\n",
    "\n",
    "                # Set seeds\n",
    "                np.random.seed(run)\n",
    "                tf.random.set_seed(run)\n",
    "\n",
    "                # Build and train\n",
    "                ae, enc = builder()\n",
    "                history = ae.fit(\n",
    "                    cls.train_images, cls.train_images,\n",
    "                    epochs=10,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Analyze\n",
    "                stats = analyze_latent_statistics(enc, cls.test_images)\n",
    "\n",
    "                cls.results[arch_name].append({\n",
    "                    'run': run,\n",
    "                    'stats': stats,\n",
    "                    'val_loss': history.history['val_loss'][-1]\n",
    "                })\n",
    "\n",
    "                print(f\"  Variance: {stats['mean_variance']:.6f}\")\n",
    "                print(f\"  Dead neurons: {stats['dead_neurons']}/{stats['total_neurons']}\")\n",
    "                print(f\"  Val loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "    def test_01_compute_statistics(self):\n",
    "        \"\"\"Compute mean, std, confidence intervals\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Statistical Summary\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        for arch_name, runs in self.results.items():\n",
    "            print(f\"\\n{arch_name}:\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "            # Extract metrics\n",
    "            variances = [r['stats']['mean_variance'] for r in runs]\n",
    "            dead_counts = [r['stats']['dead_neurons'] for r in runs]\n",
    "            losses = [r['val_loss'] for r in runs]\n",
    "\n",
    "            # Compute statistics\n",
    "            var_mean = np.mean(variances)\n",
    "            var_std = np.std(variances)\n",
    "            var_ci = 1.96 * var_std / np.sqrt(len(variances))  # 95% CI\n",
    "\n",
    "            dead_mean = np.mean(dead_counts)\n",
    "            dead_std = np.std(dead_counts)\n",
    "\n",
    "            loss_mean = np.mean(losses)\n",
    "            loss_std = np.std(losses)\n",
    "\n",
    "            print(f\"Variance:     {var_mean:.6f} ¬± {var_std:.6f} (CI: ¬±{var_ci:.6f})\")\n",
    "            print(f\"Dead neurons: {dead_mean:.1f} ¬± {dead_std:.1f}\")\n",
    "            print(f\"Val loss:     {loss_mean:.6f} ¬± {loss_std:.6f}\")\n",
    "\n",
    "            # Store for later tests\n",
    "            if not hasattr(self, 'summary'):\n",
    "                self.summary = {}\n",
    "            self.summary[arch_name] = {\n",
    "                'variance_mean': var_mean,\n",
    "                'variance_std': var_std,\n",
    "                'variance_ci': var_ci,\n",
    "                'dead_mean': dead_mean,\n",
    "                'loss_mean': loss_mean\n",
    "            }\n",
    "\n",
    "    def test_02_statistical_comparison(self):\n",
    "        \"\"\"Compare architectures with t-test\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Statistical Comparison (t-test)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        arch_names = list(self.results.keys())\n",
    "\n",
    "        if len(arch_names) >= 2:\n",
    "            arch1, arch2 = arch_names[0], arch_names[1]\n",
    "\n",
    "            vars1 = [r['stats']['mean_variance'] for r in self.results[arch1]]\n",
    "            vars2 = [r['stats']['mean_variance'] for r in self.results[arch2]]\n",
    "\n",
    "            t_stat, p_value = stats.ttest_ind(vars1, vars2)\n",
    "\n",
    "            print(f\"\\nComparing {arch1} vs {arch2}:\")\n",
    "            print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "            print(f\"  p-value: {p_value:.6f}\")\n",
    "\n",
    "            if p_value < 0.05:\n",
    "                print(f\"  ‚úì SIGNIFICANT difference (p < 0.05)\")\n",
    "                if np.mean(vars2) > np.mean(vars1):\n",
    "                    print(f\"  ‚Üí {arch2} has significantly higher variance\")\n",
    "            else:\n",
    "                print(f\"  ‚úó No significant difference (p >= 0.05)\")\n",
    "\n",
    "    def test_03_create_errorbar_plot(self):\n",
    "        \"\"\"Create plot with error bars\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Creating Error Bar Plots\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        arch_names = list(self.results.keys())\n",
    "        x = np.arange(len(arch_names))\n",
    "\n",
    "        # Collect data\n",
    "        variances_mean = []\n",
    "        variances_std = []\n",
    "        dead_mean = []\n",
    "        dead_std = []\n",
    "        loss_mean = []\n",
    "        loss_std = []\n",
    "\n",
    "        for arch in arch_names:\n",
    "            vars_list = [r['stats']['mean_variance'] for r in self.results[arch]]\n",
    "            dead_list = [r['stats']['dead_neurons'] for r in self.results[arch]]\n",
    "            loss_list = [r['val_loss'] for r in self.results[arch]]\n",
    "\n",
    "            variances_mean.append(np.mean(vars_list))\n",
    "            variances_std.append(np.std(vars_list))\n",
    "            dead_mean.append(np.mean(dead_list))\n",
    "            dead_std.append(np.std(dead_list))\n",
    "            loss_mean.append(np.mean(loss_list))\n",
    "            loss_std.append(np.std(loss_list))\n",
    "\n",
    "        # Plot 1: Variance\n",
    "        axes[0].bar(x, variances_mean, yerr=variances_std, capsize=5, alpha=0.7)\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(arch_names, rotation=45, ha='right')\n",
    "        axes[0].set_ylabel('Mean Variance')\n",
    "        axes[0].set_title('Variance Comparison\\n(with std error bars)', fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # Plot 2: Dead Neurons\n",
    "        axes[1].bar(x, dead_mean, yerr=dead_std, capsize=5, alpha=0.7, color='orange')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(arch_names, rotation=45, ha='right')\n",
    "        axes[1].set_ylabel('Dead Neurons')\n",
    "        axes[1].set_title('Dead Neurons Comparison\\n(with std error bars)', fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # Plot 3: Loss\n",
    "        axes[2].bar(x, loss_mean, yerr=loss_std, capsize=5, alpha=0.7, color='green')\n",
    "        axes[2].set_xticks(x)\n",
    "        axes[2].set_xticklabels(arch_names, rotation=45, ha='right')\n",
    "        axes[2].set_ylabel('Validation Loss (MSE)')\n",
    "        axes[2].set_title('Reconstruction Loss Comparison\\n(with std error bars)', fontweight='bold')\n",
    "        axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        plt.suptitle(f'Multiple Runs Comparison (N={self.num_runs} runs)',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('multiple_runs_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: multiple_runs_comparison.png\")\n",
    "        plt.close()\n",
    "\n",
    "class TestHenonGeneralization(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    CRITICAL EXPERIMENT #3: Henon Map Dataset\n",
    "    Test if results generalize to different chaotic system\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"HENON MAP GENERALIZATION TEST\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Generate both datasets\n",
    "        cls.logistic_train = generate_logistic_dataset(2000, fixed_initial=False)\n",
    "        cls.henon_train = generate_henon_dataset(2000)\n",
    "\n",
    "        cls.logistic_test = generate_logistic_dataset(500, fixed_initial=False)\n",
    "        cls.henon_test = generate_henon_dataset(500)\n",
    "\n",
    "        print(f\"Logistic train: {cls.logistic_train.shape}\")\n",
    "        print(f\"Henon train: {cls.henon_train.shape}\")\n",
    "\n",
    "        cls.results = {}\n",
    "\n",
    "        # Train on both datasets\n",
    "        for dataset_name, train_data, test_data in [\n",
    "            ('Logistic', cls.logistic_train, cls.logistic_test),\n",
    "            ('Henon', cls.henon_train, cls.henon_test)\n",
    "        ]:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Training K-Sparse Chaos on {dataset_name} Map\")\n",
    "            print(f\"{'='*70}\")\n",
    "\n",
    "            ae, enc = build_ksparse_chaos_ae(latent_dim=128, k_active=32)\n",
    "\n",
    "            history = ae.fit(\n",
    "                train_data, train_data,\n",
    "                epochs=10,\n",
    "                batch_size=64,\n",
    "                validation_split=0.1,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            stats = analyze_latent_statistics(enc, test_data)\n",
    "\n",
    "            cls.results[dataset_name] = {\n",
    "                'autoencoder': ae,\n",
    "                'encoder': enc,\n",
    "                'stats': stats,\n",
    "                'val_loss': history.history['val_loss'][-1]\n",
    "            }\n",
    "\n",
    "            print(f\"\\nResults:\")\n",
    "            print(f\"  Variance: {stats['mean_variance']:.6f}\")\n",
    "            print(f\"  Dead neurons: {stats['dead_neurons']}/{stats['total_neurons']}\")\n",
    "            print(f\"  Val loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "    def test_01_compare_datasets(self):\n",
    "        \"\"\"Compare results across datasets\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Logistic vs Henon Comparison\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        print(f\"\\n{'Metric':<25} {'Logistic':<15} {'Henon':<15} {'Ratio'}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        log_var = self.results['Logistic']['stats']['mean_variance']\n",
    "        hen_var = self.results['Henon']['stats']['mean_variance']\n",
    "        print(f\"{'Variance':<25} {log_var:<15.6f} {hen_var:<15.6f} {hen_var/log_var:.2f}√ó\")\n",
    "\n",
    "        log_dead = self.results['Logistic']['stats']['dead_neurons']\n",
    "        hen_dead = self.results['Henon']['stats']['dead_neurons']\n",
    "        print(f\"{'Dead Neurons':<25} {log_dead:<15} {hen_dead:<15} -\")\n",
    "\n",
    "        log_loss = self.results['Logistic']['val_loss']\n",
    "        hen_loss = self.results['Henon']['val_loss']\n",
    "        print(f\"{'Val Loss':<25} {log_loss:<15.6f} {hen_loss:<15.6f} {hen_loss/log_loss:.2f}√ó\")\n",
    "\n",
    "        # Check consistency\n",
    "        self.assertEqual(log_dead, 0, \"Logistic: should have 0 dead neurons\")\n",
    "        self.assertEqual(hen_dead, 0, \"Henon: should have 0 dead neurons\")\n",
    "\n",
    "        # Variance should be in same ballpark (within 3√ó)\n",
    "        variance_ratio = max(log_var, hen_var) / min(log_var, hen_var)\n",
    "        self.assertLess(variance_ratio, 3.0,\n",
    "                       f\"Variance too different between datasets: {variance_ratio:.2f}√ó\")\n",
    "\n",
    "    def test_02_visualize_latent_spaces(self):\n",
    "        \"\"\"Visualize and compare latent spaces\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Visualizing Latent Spaces\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "        for idx, (name, dataset) in enumerate([\n",
    "            ('Logistic', self.logistic_test),\n",
    "            ('Henon', self.henon_test)\n",
    "        ]):\n",
    "            enc = self.results[name]['encoder']\n",
    "            latents = enc.predict(dataset[:100], verbose=0)\n",
    "\n",
    "            # 2D projection\n",
    "            axes[0, idx].scatter(latents[:, 0], latents[:, 1],\n",
    "                               alpha=0.6, s=30, edgecolors='black', linewidths=0.5)\n",
    "            axes[0, idx].set_xlabel('Dim 0')\n",
    "            axes[0, idx].set_ylabel('Dim 1')\n",
    "            axes[0, idx].set_title(f'{name} Map\\nLatent Space (first 2 dims)',\n",
    "                                  fontweight='bold')\n",
    "            axes[0, idx].grid(True, alpha=0.3)\n",
    "\n",
    "            # Variance distribution\n",
    "            variance = np.var(latents, axis=0)\n",
    "            axes[1, idx].hist(np.log10(variance + 1e-10), bins=20, alpha=0.7,\n",
    "                            edgecolor='black')\n",
    "            axes[1, idx].set_xlabel('Log10(Variance)')\n",
    "            axes[1, idx].set_ylabel('Frequency')\n",
    "            axes[1, idx].set_title(f'{name} Map\\nVariance Distribution',\n",
    "                                  fontweight='bold')\n",
    "            axes[1, idx].axvline(x=np.log10(self.results[name]['stats']['mean_variance']),\n",
    "                               color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "            axes[1, idx].legend()\n",
    "            axes[1, idx].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.suptitle('Generalization Test: Logistic vs Henon Map',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('henon_generalization.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: henon_generalization.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "class TestDeadNeuronTrajectory(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    EXPERIMENT #4: Track how neurons die during training\n",
    "    \"\"\"\n",
    "\n",
    "    def test_01_track_broken_ae_death(self):\n",
    "        \"\"\"Track neuron death in broken L1 architecture\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Tracking Neuron Death (Broken L1 AE)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Build broken AE (you need to import this)\n",
    "        # For now, skip if not available\n",
    "        self.skipTest(\"Requires build_sparse_ae_broken function\")\n",
    "\n",
    "    def test_02_track_chaos_ae_stability(self):\n",
    "        \"\"\"Verify neurons stay alive in Chaos AE\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Neuron Stability (K-Sparse Chaos AE)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        images = generate_logistic_dataset(1000, fixed_initial=False)\n",
    "\n",
    "        ae, enc = build_ksparse_chaos_ae(latent_dim=128, k_active=32)\n",
    "\n",
    "        trajectory = track_dead_neurons_over_time(ae, enc, images, epochs=30)\n",
    "\n",
    "        # Plot trajectory\n",
    "        epochs = [t['epoch'] for t in trajectory]\n",
    "        dead = [t['dead_neurons'] for t in trajectory]\n",
    "        variance = [t['mean_variance'] for t in trajectory]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        axes[0].plot(epochs, dead, 'o-', linewidth=2)\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Dead Neurons')\n",
    "        axes[0].set_title('Dead Neurons Over Time', fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1].plot(epochs, variance, 'o-', linewidth=2, color='green')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Mean Variance')\n",
    "        axes[1].set_title('Variance Over Time', fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.suptitle('K-Sparse Chaos AE: Training Stability',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_stability.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: training_stability.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Assertions\n",
    "        final_dead = trajectory[-1]['dead_neurons']\n",
    "        self.assertEqual(final_dead, 0, \"Should maintain 0 dead neurons\")\n",
    "\n",
    "        final_variance = trajectory[-1]['mean_variance']\n",
    "        initial_variance = trajectory[5]['mean_variance']  # skip first few epochs\n",
    "        self.assertGreater(final_variance, initial_variance * 0.8,\n",
    "                          \"Variance should not collapse during training\")\n",
    "\n",
    "\n",
    "def run_all_critical_experiments():\n",
    "    \"\"\"Run all critical experiments for paper\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING ALL CRITICAL EXPERIMENTS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Create test suite\n",
    "    suite = unittest.TestSuite()\n",
    "\n",
    "    # Add test classes in order of priority\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestKSparseAblation))\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMultipleRuns))\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestHenonGeneralization))\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestDeadNeuronTrajectory))\n",
    "\n",
    "    # Run with detailed output\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total tests run: {result.testsRun}\")\n",
    "    print(f\"Successes: {result.testsRun - len(result.failures) - len(result.errors)}\")\n",
    "    print(f\"Failures: {len(result.failures)}\")\n",
    "    print(f\"Errors: {len(result.errors)}\")\n",
    "\n",
    "    if result.wasSuccessful():\n",
    "        print(\"\\n‚úÖ ALL CRITICAL EXPERIMENTS PASSED!\")\n",
    "        print(\"\\nGenerated files:\")\n",
    "        print(\"  - k_sparse_ablation.png\")\n",
    "        print(\"  - multiple_runs_comparison.png\")\n",
    "        print(\"  - henon_generalization.png\")\n",
    "        print(\"  - training_stability.png\")\n",
    "        print(\"\\nReady for paper! üéâ\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Some experiments failed. Review output above.\")\n",
    "\n",
    "    return result.wasSuccessful()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    success = run_all_critical_experiments()\n",
    "    exit(0 if success else 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
