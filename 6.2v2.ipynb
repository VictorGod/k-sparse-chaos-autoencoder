{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING ALL CRITICAL EXPERIMENTS (WITH FAIR BASELINE)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "K-SPARSE ABLATION STUDY\n",
      "================================================================================\n",
      "\n",
      "Testing K values: [4, 8, 16, 32, 64, 96, 112]\n",
      "Latent dimension: 128\n",
      "Sparsity range: 12.5% to 96.9%\n",
      "\n",
      "[K=4] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 14ms/step - loss: 0.1361 - val_loss: 0.1351\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1328 - val_loss: 0.1311\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1296 - val_loss: 0.1278\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1272 - val_loss: 0.1252\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1251 - val_loss: 0.1232\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1235 - val_loss: 0.1217\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1221 - val_loss: 0.1208\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1211 - val_loss: 0.1199\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1201 - val_loss: 0.1192\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1195 - val_loss: 0.1187\n",
      "  Variance: 0.068163\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 96.9%\n",
      "\n",
      "[K=8] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 25ms/step - loss: 0.1361 - val_loss: 0.1336\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1327 - val_loss: 0.1296\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1297 - val_loss: 0.1273\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1272 - val_loss: 0.1253\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1253 - val_loss: 0.1233\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1236 - val_loss: 0.1220\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1221 - val_loss: 0.1207\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1211 - val_loss: 0.1199\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1201 - val_loss: 0.1193\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1195 - val_loss: 0.1186\n",
      "  Variance: 0.130711\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 93.8%\n",
      "\n",
      "[K=16] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 15ms/step - loss: 0.1363 - val_loss: 0.1329\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1329 - val_loss: 0.1294\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1299 - val_loss: 0.1271\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1274 - val_loss: 0.1248\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1254 - val_loss: 0.1233\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1237 - val_loss: 0.1222\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1223 - val_loss: 0.1210\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1213 - val_loss: 0.1199\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1203 - val_loss: 0.1193\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1197 - val_loss: 0.1187\n",
      "  Variance: 0.238414\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 87.5%\n",
      "\n",
      "[K=32] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 13ms/step - loss: 0.1372 - val_loss: 0.1335\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1338 - val_loss: 0.1303\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1307 - val_loss: 0.1281\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1282 - val_loss: 0.1257\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1261 - val_loss: 0.1241\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1245 - val_loss: 0.1228\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1231 - val_loss: 0.1214\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1221 - val_loss: 0.1209\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1212 - val_loss: 0.1202\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1205 - val_loss: 0.1197\n",
      "  Variance: 0.417582\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 75.0%\n",
      "\n",
      "[K=64] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 12ms/step - loss: 0.1384 - val_loss: 0.1350\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1352 - val_loss: 0.1318\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1323 - val_loss: 0.1293\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1297 - val_loss: 0.1274\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1275 - val_loss: 0.1254\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1259 - val_loss: 0.1239\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1246 - val_loss: 0.1232\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1233 - val_loss: 0.1222\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1226 - val_loss: 0.1214\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1219 - val_loss: 0.1206\n",
      "  Variance: 0.571774\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 50.0%\n",
      "\n",
      "[K=96] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 30ms/step - loss: 0.1393 - val_loss: 0.1354\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.1359 - val_loss: 0.1326\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.1329 - val_loss: 0.1300\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1305 - val_loss: 0.1278\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.1284 - val_loss: 0.1260\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.1267 - val_loss: 0.1249\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 0.1252 - val_loss: 0.1240\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1240 - val_loss: 0.1228\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1233 - val_loss: 0.1222\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1225 - val_loss: 0.1214\n",
      "  Variance: 0.651265\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 25.0%\n",
      "\n",
      "[K=112] Training K-Sparse Chaos AE...\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 15ms/step - loss: 0.1395 - val_loss: 0.1356\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1358 - val_loss: 0.1322\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1329 - val_loss: 0.1301\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1304 - val_loss: 0.1278\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1283 - val_loss: 0.1262\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1266 - val_loss: 0.1247\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1252 - val_loss: 0.1238\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1241 - val_loss: 0.1228\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.1233 - val_loss: 0.1221\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1226 - val_loss: 0.1218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_01_variance_vs_k (__main__.TestKSparseAblation)\n",
      "Test how variance changes with K ... ok\n",
      "test_02_find_optimal_k (__main__.TestKSparseAblation)\n",
      "Find K that balances variance and sparsity ... ok\n",
      "test_03_create_ablation_plot (__main__.TestKSparseAblation)\n",
      "Create comprehensive ablation visualization ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variance: 0.660226\n",
      "  Dead neurons: 0/128\n",
      "  Sparsity: 12.5%\n",
      "\n",
      "================================================================================\n",
      "TEST: Variance vs K\n",
      "================================================================================\n",
      "K=  4: variance = 0.068163\n",
      "K=  8: variance = 0.130711\n",
      "K= 16: variance = 0.238414\n",
      "K= 32: variance = 0.417582\n",
      "K= 64: variance = 0.571774\n",
      "K= 96: variance = 0.651265\n",
      "K=112: variance = 0.660226\n",
      "\n",
      "Trend violations: 0/6\n",
      "\n",
      "================================================================================\n",
      "TEST: Find Optimal K\n",
      "================================================================================\n",
      "K=  4: score = 1.652 (var=0.068, sparsity=96.9%)\n",
      "K=  8: score = 1.803 (var=0.131, sparsity=93.8%)\n",
      "K= 16: score = 1.766 (var=0.238, sparsity=87.5%)\n",
      "K= 32: score = 1.606 (var=0.418, sparsity=75.0%)\n",
      "K= 64: score = 1.121 (var=0.572, sparsity=50.0%)\n",
      "K= 96: score = 0.857 (var=0.651, sparsity=25.0%)\n",
      "K=112: score = 0.746 (var=0.660, sparsity=12.5%)\n",
      "\n",
      "‚≠ê Optimal K: 8\n",
      "   Variance: 0.130711\n",
      "   Sparsity: 93.8%\n",
      "\n",
      "================================================================================\n",
      "Creating K-Sparse Ablation Plots\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved: k_sparse_ablation.png\n",
      "\n",
      "================================================================================\n",
      "ABLATION RESULTS TABLE (LaTeX format)\n",
      "================================================================================\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\begin{tabular}{ccccc}\n",
      "\\hline\n",
      "K & Sparsity & Variance & Dead Neurons & Val Loss \\\\\n",
      "\\hline\n",
      "4 & 96.9% & 0.0682 & 0/128 & 0.1187 \\\\\n",
      "8 & 93.8% & 0.1307 & 0/128 & 0.1186 \\\\\n",
      "16 & 87.5% & 0.2384 & 0/128 & 0.1187 \\\\\n",
      "32 & 75.0% & 0.4176 & 0/128 & 0.1197 \\\\\n",
      "64 & 50.0% & 0.5718 & 0/128 & 0.1206 \\\\\n",
      "96 & 25.0% & 0.6513 & 0/128 & 0.1214 \\\\\n",
      "112 & 12.5% & 0.6602 & 0/128 & 0.1218 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{K-Sparse ablation study results}\n",
      "\\end{table}\n",
      "\n",
      "================================================================================\n",
      "üéØ FAIR BASELINE COMPARISON (Dense_64 vs Dense_128 vs V4_128)\n",
      "================================================================================\n",
      "\n",
      "üìä –ó–∞–ø—É—Å–∫–∞–µ–º N=10 runs –¥–ª—è –∫–∞–∂–¥–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã...\n",
      "   Train: (2000, 28, 28, 1)\n",
      "   Test: (500, 28, 28, 1)\n",
      "\n",
      "‚ö†Ô∏è –≠—Ç–æ –∑–∞–π–º–µ—Ç ~2-3 —á–∞—Å–∞. –ù–∞–±–µ—Ä–∏—Ç–µ—Å—å —Ç–µ—Ä–ø–µ–Ω–∏—è!\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[1/3] –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Dense_ReLU_64\n",
      "======================================================================\n",
      "  [Run 1/10] var=0.0783, dead=27/64 (42%), loss=0.1137\n",
      "  [Run 2/10] var=0.1069, dead=24/64 (38%), loss=0.1136\n",
      "  [Run 3/10] var=0.0899, dead=26/64 (41%), loss=0.1135\n",
      "  [Run 4/10] var=0.1004, dead=26/64 (41%), loss=0.1134\n",
      "  [Run 5/10] var=0.0956, dead=32/64 (50%), loss=0.1135\n",
      "  [Run 6/10] var=0.0819, dead=31/64 (48%), loss=0.1137\n",
      "  [Run 7/10] var=0.0897, dead=29/64 (45%), loss=0.1133\n",
      "  [Run 8/10] var=0.0925, dead=32/64 (50%), loss=0.1135\n",
      "  [Run 9/10] var=0.0911, dead=24/64 (38%), loss=0.1133\n",
      "  [Run 10/10] var=0.0917, dead=34/64 (53%), loss=0.1136\n",
      "\n",
      "======================================================================\n",
      "[2/3] –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Dense_ReLU_128\n",
      "======================================================================\n",
      "  [Run 1/10] var=0.0639, dead=52/128 (41%), loss=0.1130\n",
      "  [Run 2/10] var=0.0616, dead=47/128 (37%), loss=0.1135\n",
      "  [Run 3/10] var=0.0670, dead=42/128 (33%), loss=0.1132\n",
      "  [Run 4/10] var=0.0550, dead=34/128 (27%), loss=0.1134\n",
      "  [Run 5/10] var=0.0555, dead=47/128 (37%), loss=0.1130\n",
      "  [Run 6/10] var=0.0606, dead=46/128 (36%), loss=0.1133\n",
      "  [Run 7/10] var=0.0529, dead=57/128 (45%), loss=0.1131\n",
      "  [Run 8/10] var=0.0718, dead=41/128 (32%), loss=0.1132\n",
      "  [Run 9/10] var=0.0658, dead=50/128 (39%), loss=0.1131\n",
      "  [Run 10/10] var=0.0650, dead=37/128 (29%), loss=0.1131\n",
      "\n",
      "======================================================================\n",
      "[3/3] –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: V4_KSparse_128\n",
      "======================================================================\n",
      "  [Run 1/10] var=0.4160, dead=0/128 (0%), loss=0.1198\n",
      "  [Run 2/10] var=0.4198, dead=0/128 (0%), loss=0.1198\n",
      "  [Run 3/10] var=0.4178, dead=0/128 (0%), loss=0.1200\n",
      "  [Run 4/10] var=0.4203, dead=0/128 (0%), loss=0.1199\n",
      "  [Run 5/10] var=0.4132, dead=0/128 (0%), loss=0.1199\n",
      "  [Run 6/10] var=0.4160, dead=0/128 (0%), loss=0.1197\n",
      "  [Run 7/10] var=0.4212, dead=0/128 (0%), loss=0.1200\n",
      "  [Run 8/10] var=0.4174, dead=0/128 (0%), loss=0.1199\n",
      "  [Run 9/10] var=0.4176, dead=0/128 (0%), loss=0.1198\n",
      "  [Run 10/10] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_01_compute_statistics (__main__.TestFairBaselineComparison)\n",
      "Compute mean, std, confidence intervals ... ok\n",
      "test_02_fair_statistical_comparison (__main__.TestFairBaselineComparison)\n",
      "Compare Dense_128 vs V4_128 (FAIR) ... ERROR\n",
      "test_03_create_fair_comparison_plots (__main__.TestFairBaselineComparison)\n",
      "Create detailed comparison visualization ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var=0.4180, dead=0/128 (0%), loss=0.1198\n",
      "\n",
      "================================================================================\n",
      "TEST: Statistical Summary\n",
      "================================================================================\n",
      "\n",
      "Dense_ReLU_64:\n",
      "==================================================\n",
      "Variance:     0.091792 ¬± 0.007784 (CI: ¬±0.004825)\n",
      "Dead neurons: 28.5 ¬± 3.4 (44.5%)\n",
      "Val loss:     0.113511 ¬± 0.000145\n",
      "\n",
      "Dense_ReLU_128:\n",
      "==================================================\n",
      "Variance:     0.061902 ¬± 0.005692 (CI: ¬±0.003528)\n",
      "Dead neurons: 45.3 ¬± 6.6 (35.4%)\n",
      "Val loss:     0.113181 ¬± 0.000160\n",
      "\n",
      "V4_KSparse_128:\n",
      "==================================================\n",
      "Variance:     0.417723 ¬± 0.002239 (CI: ¬±0.001388)\n",
      "Dead neurons: 0.0 ¬± 0.0 (0.0%)\n",
      "Val loss:     0.119851 ¬± 0.000091\n",
      "\n",
      "================================================================================\n",
      "TEST: FAIR Statistical Comparison (t-tests)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Creating Fair Comparison Plots\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_04_create_latex_table (__main__.TestFairBaselineComparison)\n",
      "Create LaTeX table for paper ... ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved: fair_baseline_comparison.png\n",
      "\n",
      "================================================================================\n",
      "LATEX TABLE FOR PAPER (Fair Comparison)\n",
      "================================================================================\n",
      "\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Fair Baseline Comparison on Logistic Map Dataset}\n",
      "\\label{tab:fair_comparison}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\hline\n",
      "Architecture & Latent Dim & Variance & Dead Neurons & Val Loss \\\\\n",
      "\\hline\n",
      "Dense ReLU & 64 & $0.092 \\pm 0.008$ & 44.5% & $0.1135 \\pm 0.0001$ \\\\\n",
      "Dense ReLU (fair) & 128 & $0.062 \\pm 0.006$ & 35.4% & $0.1132 \\pm 0.0002$ \\\\\n",
      "\\textbf{K-Sparse Chaos (V4)} & 128 & $0.418 \\pm 0.002$ & 0.0% & $0.1199 \\pm 0.0001$ \\\\\n",
      "\\hline\n",
      "\\hline\n",
      "\\multicolumn{5}{l}{\\textit{Fair improvement (V4 vs Dense-128): 6.75√ó variance, 0% vs 35% dead neurons}} \\\\\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "================================================================================\n",
      "üìã –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç—É —Ç–∞–±–ª–∏—Ü—É –≤ –≤–∞—à—É —Å—Ç–∞—Ç—å—é!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "MULTIPLE RUNS FOR STATISTICAL SIGNIFICANCE (Legacy)\n",
      "================================================================================\n",
      "‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ TestFairBaselineComparison –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
      "\n",
      "======================================================================\n",
      "Architecture: Dense_ReLU\n",
      "======================================================================\n",
      "\n",
      "[Run 1/5]\n",
      "  Variance: 0.070269\n",
      "  Dead neurons: 26/64\n",
      "  Val loss: 0.113756\n",
      "\n",
      "[Run 2/5]\n",
      "  Variance: 0.071937\n",
      "  Dead neurons: 35/64\n",
      "  Val loss: 0.113796\n",
      "\n",
      "[Run 3/5]\n",
      "  Variance: 0.066056\n",
      "  Dead neurons: 29/64\n",
      "  Val loss: 0.113853\n",
      "\n",
      "[Run 4/5]\n",
      "  Variance: 0.091161\n",
      "  Dead neurons: 31/64\n",
      "  Val loss: 0.113894\n",
      "\n",
      "[Run 5/5]\n",
      "  Variance: 0.080841\n",
      "  Dead neurons: 25/64\n",
      "  Val loss: 0.113622\n",
      "\n",
      "======================================================================\n",
      "Architecture: KSparse_Chaos\n",
      "======================================================================\n",
      "\n",
      "[Run 1/5]\n",
      "  Variance: 0.419032\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.119926\n",
      "\n",
      "[Run 2/5]\n",
      "  Variance: 0.418010\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.119783\n",
      "\n",
      "[Run 3/5]\n",
      "  Variance: 0.417632\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.119825\n",
      "\n",
      "[Run 4/5]\n",
      "  Variance: 0.418775\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.120020\n",
      "\n",
      "[Run 5/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_01_compute_statistics (__main__.TestMultipleRuns)\n",
      "Compute mean, std, confidence intervals ... ok\n",
      "test_02_statistical_comparison (__main__.TestMultipleRuns)\n",
      "Compare architectures with t-test ... ok\n",
      "test_03_create_errorbar_plot (__main__.TestMultipleRuns)\n",
      "Create plot with error bars ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variance: 0.420392\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.119869\n",
      "\n",
      "================================================================================\n",
      "TEST: Statistical Summary\n",
      "================================================================================\n",
      "\n",
      "Dense_ReLU:\n",
      "==================================================\n",
      "Variance:     0.076053 ¬± 0.008961 (CI: ¬±0.007855)\n",
      "Dead neurons: 29.2 ¬± 3.6\n",
      "Val loss:     0.113784 ¬± 0.000094\n",
      "\n",
      "KSparse_Chaos:\n",
      "==================================================\n",
      "Variance:     0.418768 ¬± 0.000956 (CI: ¬±0.000838)\n",
      "Dead neurons: 0.0 ¬± 0.0\n",
      "Val loss:     0.119885 ¬± 0.000083\n",
      "\n",
      "================================================================================\n",
      "TEST: Statistical Comparison (t-test)\n",
      "================================================================================\n",
      "\n",
      "Comparing Dense_ReLU vs KSparse_Chaos:\n",
      "  t-statistic: -76.0567\n",
      "  p-value: 0.000000\n",
      "  ‚úì SIGNIFICANT difference (p < 0.05)\n",
      "  ‚Üí KSparse_Chaos has significantly higher variance\n",
      "\n",
      "================================================================================\n",
      "Creating Error Bar Plots\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved: multiple_runs_comparison.png\n",
      "\n",
      "================================================================================\n",
      "HENON MAP GENERALIZATION TEST\n",
      "================================================================================\n",
      "Logistic train: (2000, 28, 28, 1)\n",
      "Henon train: (2000, 28, 28, 1)\n",
      "\n",
      "======================================================================\n",
      "Training K-Sparse Chaos on Logistic Map\n",
      "======================================================================\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 21ms/step - loss: 0.1370 - val_loss: 0.1339\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1334 - val_loss: 0.1309\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.1306 - val_loss: 0.1286\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.1281 - val_loss: 0.1263\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.1260 - val_loss: 0.1246\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1244 - val_loss: 0.1234\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 0.1230 - val_loss: 0.1221\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 19ms/step - loss: 0.1220 - val_loss: 0.1213\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 27ms/step - loss: 0.1211 - val_loss: 0.1206\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.1204 - val_loss: 0.1201\n",
      "\n",
      "Results:\n",
      "  Variance: 0.417578\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.120130\n",
      "\n",
      "======================================================================\n",
      "Training K-Sparse Chaos on Henon Map\n",
      "======================================================================\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 23ms/step - loss: 0.0774 - val_loss: 0.0731\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0693 - val_loss: 0.0664\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.0614 - val_loss: 0.0611\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0566 - val_loss: 0.0524\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.0544 - val_loss: 0.0554\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0525 - val_loss: 0.0482\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0508 - val_loss: 0.0507\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0490 - val_loss: 0.0509\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0475 - val_loss: 0.0475\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.0470 - val_loss: 0.0456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_01_compare_datasets (__main__.TestHenonGeneralization)\n",
      "Compare results across datasets ... ok\n",
      "test_02_visualize_latent_spaces (__main__.TestHenonGeneralization)\n",
      "Visualize and compare latent spaces ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Variance: 0.422372\n",
      "  Dead neurons: 0/128\n",
      "  Val loss: 0.045649\n",
      "\n",
      "================================================================================\n",
      "TEST: Logistic vs Henon Comparison\n",
      "================================================================================\n",
      "\n",
      "Metric                    Logistic        Henon           Ratio\n",
      "------------------------------------------------------------\n",
      "Variance                  0.417578        0.422372        1.01√ó\n",
      "Dead Neurons              0               0               -\n",
      "Val Loss                  0.120130        0.045649        0.38√ó\n",
      "\n",
      "================================================================================\n",
      "Visualizing Latent Spaces\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_01_track_broken_ae_death (__main__.TestDeadNeuronTrajectory)\n",
      "Track neuron death in broken L1 architecture ... skipped 'Requires build_sparse_ae_broken function'\n",
      "test_02_track_chaos_ae_stability (__main__.TestDeadNeuronTrajectory)\n",
      "Verify neurons stay alive in Chaos AE ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved: henon_generalization.png\n",
      "\n",
      "================================================================================\n",
      "TEST: Tracking Neuron Death (Broken L1 AE)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TEST: Neuron Stability (K-Sparse Chaos AE)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_02_fair_statistical_comparison (__main__.TestFairBaselineComparison)\n",
      "Compare Dense_128 vs V4_128 (FAIR)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5884\\3247979534.py\", line 587, in test_02_fair_statistical_comparison\n",
      "    if 'Dense_ReLU_64' in self.summary and 'Dense_ReLU_128' in self.summary:\n",
      "AttributeError: 'TestFairBaselineComparison' object has no attribute 'summary'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 14 tests in 275.970s\n",
      "\n",
      "FAILED (errors=1, skipped=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved: training_stability.png\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT SUMMARY\n",
      "================================================================================\n",
      "Total tests run: 14\n",
      "Successes: 13\n",
      "Failures: 0\n",
      "Errors: 1\n",
      "\n",
      "‚ö†Ô∏è Some experiments failed. Review output above.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import unittest\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "def logistic_map(x, r=3.99):\n",
    "    return r * x * (1 - x)\n",
    "\n",
    "def generate_logistic_map_image(image_size=28, initial_value=0.4, r=3.99):\n",
    "    iterations = image_size * image_size\n",
    "    x = initial_value\n",
    "    seq = []\n",
    "    for _ in range(iterations):\n",
    "        x = logistic_map(x, r)\n",
    "        seq.append(x)\n",
    "    img = np.array(seq).reshape((image_size, image_size))\n",
    "    return img\n",
    "\n",
    "def generate_logistic_dataset(num_images, image_size=28, r=3.99, fixed_initial=False):\n",
    "    dataset = []\n",
    "    for _ in range(num_images):\n",
    "        init_val = 0.4 if fixed_initial else np.random.rand()\n",
    "        img = generate_logistic_map_image(image_size=image_size, initial_value=init_val, r=r)\n",
    "        dataset.append(img)\n",
    "    return np.array(dataset)[..., np.newaxis].astype('float32')\n",
    "\n",
    "def henon_map(x, y, a=1.4, b=0.3):\n",
    "    \"\"\"Henon attractor - 2D chaotic system\"\"\"\n",
    "    x = np.clip(x, -2.0, 2.0)\n",
    "    y = np.clip(y, -2.0, 2.0)\n",
    "    \n",
    "    x_new = 1 - a * x**2 + y\n",
    "    y_new = b * x\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def generate_henon_image(image_size=28, x0=0.1, y0=0.1, a=1.4, b=0.3):\n",
    "    \"\"\"Generate image from Henon attractor trajectory\"\"\"\n",
    "    iterations = image_size * image_size\n",
    "    x, y = x0, y0\n",
    "    points = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        x, y = henon_map(x, y, a, b)\n",
    "        # Normalize to [0, 1] (Henon values roughly in [-1.5, 1.5])\n",
    "        normalized = (x + 2.0) / 4.0\n",
    "        points.append(np.clip(normalized, 0, 1))\n",
    "\n",
    "    img = np.array(points).reshape((image_size, image_size))\n",
    "    return img\n",
    "\n",
    "def generate_henon_dataset(num_images, image_size=28, a=1.4, b=0.3):\n",
    "    dataset = []\n",
    "    for _ in range(num_images):\n",
    "        x0 = np.random.uniform(-0.5, 0.5)\n",
    "        y0 = np.random.uniform(-0.5, 0.5)\n",
    "        img = generate_henon_image(image_size, x0, y0, a, b)\n",
    "        dataset.append(img)\n",
    "    return np.array(dataset)[..., np.newaxis].astype('float32')\n",
    "\n",
    "\n",
    "class KSparseLayer(layers.Layer):\n",
    "    def __init__(self, k=32, **kwargs):\n",
    "        super(KSparseLayer, self).__init__(**kwargs)\n",
    "        self.k = k\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        latent_dim = tf.shape(inputs)[1]\n",
    "        values, indices = tf.nn.top_k(tf.abs(inputs), k=self.k, sorted=False)\n",
    "        mask = tf.reduce_sum(\n",
    "            tf.one_hot(indices, latent_dim, dtype=inputs.dtype),\n",
    "            axis=1\n",
    "        )\n",
    "        return inputs * mask\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"k\": self.k})\n",
    "        return config\n",
    "\n",
    "class TargetVarianceRegularizer(layers.Layer):\n",
    "    def __init__(self, lambda_reg=0.01, target_variance=0.1, **kwargs):\n",
    "        super(TargetVarianceRegularizer, self).__init__(**kwargs)\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.target_variance = target_variance\n",
    "\n",
    "    def call(self, inputs):\n",
    "        current_variance = tf.math.reduce_variance(inputs, axis=0)\n",
    "        mean_variance = tf.reduce_mean(current_variance)\n",
    "        variance_penalty = self.lambda_reg * tf.square(\n",
    "            mean_variance - self.target_variance\n",
    "        )\n",
    "        self.add_loss(variance_penalty)\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"lambda_reg\": self.lambda_reg,\n",
    "            \"target_variance\": self.target_variance\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def chaos_activation(x):\n",
    "    return tf.sin(8.0 * x) + 0.5 * tf.tanh(4.0 * x)\n",
    "\n",
    "\n",
    "def build_ksparse_chaos_ae(image_size=(28, 28), latent_dim=128, k_active=32):\n",
    "    \"\"\"K-Sparse Chaos autoencoder\"\"\"\n",
    "    input_img = keras.Input(shape=(*image_size, 1))\n",
    "    x = layers.Flatten()(input_img)\n",
    "\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.Activation(chaos_activation)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    latent_pre = layers.Dense(latent_dim, name='latent_pre')(x)\n",
    "    latent_pre = layers.Activation(chaos_activation)(latent_pre)\n",
    "\n",
    "    latent = KSparseLayer(k=k_active, name='latent_ksparse')(latent_pre)\n",
    "    latent = TargetVarianceRegularizer(\n",
    "        lambda_reg=0.01,\n",
    "        target_variance=0.1\n",
    "    )(latent)\n",
    "\n",
    "    encoder = keras.Model(input_img, latent, name='ksparse_chaos_encoder')\n",
    "\n",
    "    x = layers.Dense(256)(latent)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(chaos_activation)(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    decoded = layers.Dense(np.prod(image_size), activation='sigmoid')(x)\n",
    "    decoded = layers.Reshape((*image_size, 1))(decoded)\n",
    "\n",
    "    autoencoder = keras.Model(input_img, decoded, name='ksparse_chaos_autoencoder')\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def build_dense_relu_ae(image_size=(28, 28), latent_dim=64):\n",
    "    \"\"\"Dense ReLU baseline\"\"\"\n",
    "    h, w = image_size\n",
    "    input_img = keras.Input(shape=(h, w, 1))\n",
    "    x = layers.Flatten()(input_img)\n",
    "\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    latent = layers.Dense(latent_dim, activation=\"relu\", name=\"latent\")(x)\n",
    "\n",
    "    encoder = keras.Model(input_img, latent, name=\"dense_encoder\")\n",
    "\n",
    "    x = layers.Dense(128, activation=\"relu\")(latent)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dense(h * w, activation=\"sigmoid\")(x)\n",
    "    decoded = layers.Reshape((h, w, 1))(x)\n",
    "\n",
    "    autoencoder = keras.Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "\n",
    "def analyze_latent_statistics(encoder, images, zero_threshold=1e-6):\n",
    "    \"\"\"Comprehensive latent space analysis\"\"\"\n",
    "    latents = encoder.predict(images, verbose=0)\n",
    "\n",
    "    # Variance\n",
    "    variance_per_dim = np.var(latents, axis=0)\n",
    "    mean_variance = float(np.mean(variance_per_dim))\n",
    "\n",
    "    # Dead neurons\n",
    "    dead_mask = np.all(np.abs(latents) < zero_threshold, axis=0)\n",
    "    dead_neurons = int(np.sum(dead_mask))\n",
    "    total_neurons = latents.shape[1]\n",
    "\n",
    "    # Sparsity\n",
    "    zero_mask = np.abs(latents) < zero_threshold\n",
    "    overall_sparsity = np.mean(zero_mask)\n",
    "\n",
    "    # Active neurons statistics\n",
    "    active_per_sample = np.sum(~zero_mask, axis=1)\n",
    "    mean_active = np.mean(active_per_sample)\n",
    "\n",
    "    # Variance of active neurons only\n",
    "    variance_active = []\n",
    "    for dim in range(latents.shape[1]):\n",
    "        dim_values = latents[:, dim]\n",
    "        active_mask = ~zero_mask[:, dim]\n",
    "        if np.sum(active_mask) > 1:\n",
    "            active_values = dim_values[active_mask]\n",
    "            variance_active.append(np.var(active_values))\n",
    "\n",
    "    mean_variance_active = np.mean(variance_active) if variance_active else 0.0\n",
    "\n",
    "    return {\n",
    "        'variance_per_dim': variance_per_dim,\n",
    "        'mean_variance': mean_variance,\n",
    "        'dead_neurons': dead_neurons,\n",
    "        'total_neurons': total_neurons,\n",
    "        'dead_percentage': dead_neurons / total_neurons,\n",
    "        'overall_sparsity': overall_sparsity,\n",
    "        'mean_active_neurons': mean_active,\n",
    "        'mean_variance_active': mean_variance_active,\n",
    "        'latents': latents\n",
    "    }\n",
    "\n",
    "def track_dead_neurons_over_time(model, encoder, images, epochs=50, batch_size=64):\n",
    "    \"\"\"Track how neurons die during training\"\"\"\n",
    "    trajectory = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train one epoch\n",
    "        model.fit(images, images, epochs=1, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Analyze current state\n",
    "        stats = analyze_latent_statistics(encoder, images[:200])\n",
    "\n",
    "        trajectory.append({\n",
    "            'epoch': epoch,\n",
    "            'dead_neurons': stats['dead_neurons'],\n",
    "            'mean_variance': stats['mean_variance'],\n",
    "            'sparsity': stats['overall_sparsity']\n",
    "        })\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "class TestKSparseAblation(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    CRITICAL EXPERIMENT #1: K-Sparse Ablation Study\n",
    "    Test different K values to find optimal sparsity\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"K-SPARSE ABLATION STUDY\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Generate data once\n",
    "        cls.train_images = generate_logistic_dataset(2000, fixed_initial=False)\n",
    "        cls.test_images = generate_logistic_dataset(500, fixed_initial=False)\n",
    "\n",
    "        # Test different K values\n",
    "        cls.k_values = [4, 8, 16, 32, 64, 96, 112]\n",
    "        cls.latent_dim = 128\n",
    "        cls.results = {}\n",
    "\n",
    "        print(f\"\\nTesting K values: {cls.k_values}\")\n",
    "        print(f\"Latent dimension: {cls.latent_dim}\")\n",
    "        print(f\"Sparsity range: {(cls.latent_dim - max(cls.k_values))/cls.latent_dim:.1%} to {(cls.latent_dim - min(cls.k_values))/cls.latent_dim:.1%}\")\n",
    "\n",
    "        # Train models for each K\n",
    "        for k in cls.k_values:\n",
    "            print(f\"\\n[K={k}] Training K-Sparse Chaos AE...\")\n",
    "            ae, enc = build_ksparse_chaos_ae(\n",
    "                latent_dim=cls.latent_dim,\n",
    "                k_active=k\n",
    "            )\n",
    "\n",
    "            history = ae.fit(\n",
    "                cls.train_images, cls.train_images,\n",
    "                epochs=10,\n",
    "                batch_size=64,\n",
    "                validation_split=0.1,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Analyze\n",
    "            stats = analyze_latent_statistics(enc, cls.test_images)\n",
    "\n",
    "            cls.results[k] = {\n",
    "                'autoencoder': ae,\n",
    "                'encoder': enc,\n",
    "                'stats': stats,\n",
    "                'val_loss': history.history['val_loss'][-1],\n",
    "                'sparsity': (cls.latent_dim - k) / cls.latent_dim\n",
    "            }\n",
    "\n",
    "            print(f\"  Variance: {stats['mean_variance']:.6f}\")\n",
    "            print(f\"  Dead neurons: {stats['dead_neurons']}/{cls.latent_dim}\")\n",
    "            print(f\"  Sparsity: {cls.results[k]['sparsity']:.1%}\")\n",
    "\n",
    "    def test_01_variance_vs_k(self):\n",
    "        \"\"\"Test how variance changes with K\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Variance vs K\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        for k in self.k_values:\n",
    "            variance = self.results[k]['stats']['mean_variance']\n",
    "            print(f\"K={k:3d}: variance = {variance:.6f}\")\n",
    "\n",
    "        # Check that variance generally increases with K\n",
    "        variances = [self.results[k]['stats']['mean_variance'] for k in self.k_values]\n",
    "\n",
    "        # At least monotonic trend (allowing some noise)\n",
    "        trend_violations = 0\n",
    "        for i in range(len(variances)-1):\n",
    "            if variances[i+1] < variances[i]:\n",
    "                trend_violations += 1\n",
    "\n",
    "        print(f\"\\nTrend violations: {trend_violations}/{len(variances)-1}\")\n",
    "        self.assertLess(trend_violations, len(variances)//2,\n",
    "                       \"Variance should generally increase with K\")\n",
    "\n",
    "    def test_02_find_optimal_k(self):\n",
    "        \"\"\"Find K that balances variance and sparsity\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Find Optimal K\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Define metric: variance / (1 - sparsity)\n",
    "        # Higher = better efficiency\n",
    "        scores = {}\n",
    "        for k in self.k_values:\n",
    "            variance = self.results[k]['stats']['mean_variance']\n",
    "            sparsity = self.results[k]['sparsity']\n",
    "            score = variance / (1 - sparsity + 0.01)  # avoid division by zero\n",
    "            scores[k] = score\n",
    "            print(f\"K={k:3d}: score = {score:.3f} (var={variance:.3f}, sparsity={sparsity:.1%})\")\n",
    "\n",
    "        optimal_k = max(scores, key=scores.get)\n",
    "        print(f\"\\n‚≠ê Optimal K: {optimal_k}\")\n",
    "        print(f\"   Variance: {self.results[optimal_k]['stats']['mean_variance']:.6f}\")\n",
    "        print(f\"   Sparsity: {self.results[optimal_k]['sparsity']:.1%}\")\n",
    "\n",
    "        # Save result\n",
    "        self.optimal_k = optimal_k\n",
    "\n",
    "    def test_03_create_ablation_plot(self):\n",
    "        \"\"\"Create comprehensive ablation visualization\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Creating K-Sparse Ablation Plots\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "        k_vals = self.k_values\n",
    "        variances = [self.results[k]['stats']['mean_variance'] for k in k_vals]\n",
    "        sparsities = [self.results[k]['sparsity'] for k in k_vals]\n",
    "        dead = [self.results[k]['stats']['dead_neurons'] for k in k_vals]\n",
    "        losses = [self.results[k]['val_loss'] for k in k_vals]\n",
    "\n",
    "        # Plot 1: Variance vs K\n",
    "        axes[0, 0].plot(k_vals, variances, 'o-', linewidth=2, markersize=8)\n",
    "        axes[0, 0].set_xlabel('K (Active Neurons)')\n",
    "        axes[0, 0].set_ylabel('Mean Variance')\n",
    "        axes[0, 0].set_title('Variance vs K', fontweight='bold')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].axvline(x=32, color='red', linestyle='--', alpha=0.5, label='K=32 (your choice)')\n",
    "        axes[0, 0].legend()\n",
    "\n",
    "        # Plot 2: Sparsity vs Variance\n",
    "        axes[0, 1].plot(sparsities, variances, 'o-', linewidth=2, markersize=8)\n",
    "        axes[0, 1].set_xlabel('Sparsity')\n",
    "        axes[0, 1].set_ylabel('Mean Variance')\n",
    "        axes[0, 1].set_title('Sparsity-Variance Trade-off', fontweight='bold')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Annotate K=32\n",
    "        idx_32 = k_vals.index(32)\n",
    "        axes[0, 1].annotate('K=32',\n",
    "                           xy=(sparsities[idx_32], variances[idx_32]),\n",
    "                           xytext=(10, 10), textcoords='offset points',\n",
    "                           bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "                           arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "        # Plot 3: Dead Neurons vs K\n",
    "        axes[1, 0].bar(range(len(k_vals)), dead, color='steelblue', alpha=0.7)\n",
    "        axes[1, 0].set_xticks(range(len(k_vals)))\n",
    "        axes[1, 0].set_xticklabels(k_vals)\n",
    "        axes[1, 0].set_xlabel('K (Active Neurons)')\n",
    "        axes[1, 0].set_ylabel('Dead Neurons')\n",
    "        axes[1, 0].set_title('Dead Neurons vs K', fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # Plot 4: Reconstruction Loss vs K\n",
    "        axes[1, 1].plot(k_vals, losses, 'o-', linewidth=2, markersize=8, color='green')\n",
    "        axes[1, 1].set_xlabel('K (Active Neurons)')\n",
    "        axes[1, 1].set_ylabel('Validation Loss (MSE)')\n",
    "        axes[1, 1].set_title('Reconstruction Quality vs K', fontweight='bold')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.suptitle('K-Sparse Ablation Study: Finding Optimal Sparsity',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('k_sparse_ablation.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: k_sparse_ablation.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Also create a summary table\n",
    "        self.create_ablation_table()\n",
    "\n",
    "    def create_ablation_table(self):\n",
    "        \"\"\"Create LaTeX-ready table\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ABLATION RESULTS TABLE (LaTeX format)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        print(\"\\\\begin{table}[h]\")\n",
    "        print(\"\\\\centering\")\n",
    "        print(\"\\\\begin{tabular}{ccccc}\")\n",
    "        print(\"\\\\hline\")\n",
    "        print(\"K & Sparsity & Variance & Dead Neurons & Val Loss \\\\\\\\\")\n",
    "        print(\"\\\\hline\")\n",
    "\n",
    "        for k in self.k_values:\n",
    "            r = self.results[k]\n",
    "            print(f\"{k} & {r['sparsity']:.1%} & {r['stats']['mean_variance']:.4f} & \"\n",
    "                  f\"{r['stats']['dead_neurons']}/{self.latent_dim} & {r['val_loss']:.4f} \\\\\\\\\")\n",
    "\n",
    "        print(\"\\\\hline\")\n",
    "        print(\"\\\\end{tabular}\")\n",
    "        print(\"\\\\caption{K-Sparse ablation study results}\")\n",
    "        print(\"\\\\end{table}\")\n",
    "\n",
    "\n",
    "class TestFairBaselineComparison(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    CRITICAL EXPERIMENT #2: FAIR BASELINE COMPARISON\n",
    "    Compare Dense_64 vs Dense_128 vs V4_128 (—á–µ—Å—Ç–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ!)\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üéØ FAIR BASELINE COMPARISON (Dense_64 vs Dense_128 vs V4_128)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        cls.num_runs = 10  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 5 –¥–æ 10 –¥–ª—è –ª—É—á—à–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "        cls.architectures = {\n",
    "            'Dense_ReLU_64': lambda: build_dense_relu_ae(latent_dim=64),\n",
    "            'Dense_ReLU_128': lambda: build_dense_relu_ae(latent_dim=128),  # ‚Üê –ù–û–í–û–ï!\n",
    "            'V4_KSparse_128': lambda: build_ksparse_chaos_ae(latent_dim=128, k_active=32)\n",
    "        }\n",
    "\n",
    "        cls.results = {name: [] for name in cls.architectures.keys()}\n",
    "\n",
    "        # Generate data once\n",
    "        cls.train_images = generate_logistic_dataset(2000, fixed_initial=False)\n",
    "        cls.test_images = generate_logistic_dataset(500, fixed_initial=False)\n",
    "\n",
    "        print(f\"\\nüìä –ó–∞–ø—É—Å–∫–∞–µ–º N={cls.num_runs} runs –¥–ª—è –∫–∞–∂–¥–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã...\")\n",
    "        print(f\"   Train: {cls.train_images.shape}\")\n",
    "        print(f\"   Test: {cls.test_images.shape}\")\n",
    "        print(\"\\n‚ö†Ô∏è –≠—Ç–æ –∑–∞–π–º–µ—Ç ~2-3 —á–∞—Å–∞. –ù–∞–±–µ—Ä–∏—Ç–µ—Å—å —Ç–µ—Ä–ø–µ–Ω–∏—è!\\n\")\n",
    "\n",
    "        # Run experiments\n",
    "        for arch_idx, (arch_name, builder) in enumerate(cls.architectures.items(), 1):\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"[{arch_idx}/3] –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {arch_name}\")\n",
    "            print(f\"{'='*70}\")\n",
    "\n",
    "            for run in range(cls.num_runs):\n",
    "                print(f\"  [Run {run+1}/{cls.num_runs}] \", end=\"\")\n",
    "\n",
    "                # Set seeds\n",
    "                np.random.seed(run)\n",
    "                tf.random.set_seed(run)\n",
    "\n",
    "                # Build and train\n",
    "                ae, enc = builder()\n",
    "                history = ae.fit(\n",
    "                    cls.train_images, cls.train_images,\n",
    "                    epochs=10,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=0  # —Ç–∏—Ö–∏–π —Ä–µ–∂–∏–º\n",
    "                )\n",
    "\n",
    "                # Analyze\n",
    "                stats = analyze_latent_statistics(enc, cls.test_images)\n",
    "\n",
    "                cls.results[arch_name].append({\n",
    "                    'run': run,\n",
    "                    'variance': stats['mean_variance'],\n",
    "                    'dead_neurons': stats['dead_neurons'],\n",
    "                    'total_neurons': stats['total_neurons'],\n",
    "                    'dead_percentage': stats['dead_percentage'],\n",
    "                    'val_loss': history.history['val_loss'][-1]\n",
    "                })\n",
    "\n",
    "                print(f\"var={stats['mean_variance']:.4f}, \"\n",
    "                      f\"dead={stats['dead_neurons']}/{stats['total_neurons']} \"\n",
    "                      f\"({stats['dead_percentage']:.0%}), \"\n",
    "                      f\"loss={history.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "    def test_01_compute_statistics(self):\n",
    "        \"\"\"Compute mean, std, confidence intervals\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Statistical Summary\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        summary = {}\n",
    "\n",
    "        for arch_name, runs in self.results.items():\n",
    "            print(f\"\\n{arch_name}:\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "            # Extract metrics\n",
    "            variances = [r['variance'] for r in runs]\n",
    "            dead_counts = [r['dead_neurons'] for r in runs]\n",
    "            dead_pcts = [r['dead_percentage'] for r in runs]\n",
    "            losses = [r['val_loss'] for r in runs]\n",
    "\n",
    "            # Compute statistics\n",
    "            var_mean = np.mean(variances)\n",
    "            var_std = np.std(variances)\n",
    "            var_ci = 1.96 * var_std / np.sqrt(len(variances))  # 95% CI\n",
    "\n",
    "            dead_mean = np.mean(dead_counts)\n",
    "            dead_std = np.std(dead_counts)\n",
    "            dead_pct_mean = np.mean(dead_pcts)\n",
    "\n",
    "            loss_mean = np.mean(losses)\n",
    "            loss_std = np.std(losses)\n",
    "\n",
    "            print(f\"Variance:     {var_mean:.6f} ¬± {var_std:.6f} (CI: ¬±{var_ci:.6f})\")\n",
    "            print(f\"Dead neurons: {dead_mean:.1f} ¬± {dead_std:.1f} ({dead_pct_mean:.1%})\")\n",
    "            print(f\"Val loss:     {loss_mean:.6f} ¬± {loss_std:.6f}\")\n",
    "\n",
    "            summary[arch_name] = {\n",
    "                'variances': variances,\n",
    "                'dead_pcts': dead_pcts,\n",
    "                'var_mean': var_mean,\n",
    "                'var_std': var_std,\n",
    "                'var_ci': var_ci,\n",
    "                'dead_mean': dead_mean,\n",
    "                'dead_pct_mean': dead_pct_mean,\n",
    "                'loss_mean': loss_mean\n",
    "            }\n",
    "\n",
    "        self.summary = summary\n",
    "\n",
    "    def test_02_fair_statistical_comparison(self):\n",
    "        \"\"\"Compare Dense_128 vs V4_128 (FAIR)\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: FAIR Statistical Comparison (t-tests)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # 1. Dense_64 vs Dense_128 (–≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏)\n",
    "        if 'Dense_ReLU_64' in self.summary and 'Dense_ReLU_128' in self.summary:\n",
    "            print(\"\\n1. Dense_64 vs Dense_128 (–í–ª–∏—è–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏):\")\n",
    "            print(\"   \" + \"-\"*60)\n",
    "\n",
    "            vars_64 = self.summary['Dense_ReLU_64']['variances']\n",
    "            vars_128 = self.summary['Dense_ReLU_128']['variances']\n",
    "\n",
    "            t_stat, p_value = stats.ttest_ind(vars_64, vars_128)\n",
    "\n",
    "            mean_64 = np.mean(vars_64)\n",
    "            mean_128 = np.mean(vars_128)\n",
    "            improvement = mean_128 / mean_64\n",
    "\n",
    "            print(f\"   Variance: {mean_64:.4f} ‚Üí {mean_128:.4f}\")\n",
    "            print(f\"   t-statistic: {t_stat:.4f}\")\n",
    "            print(f\"   p-value: {p_value:.6f}\")\n",
    "\n",
    "            if p_value < 0.05:\n",
    "                print(f\"   ‚úì –ó–ù–ê–ß–ò–ú–û: Dense_128 –∏–º–µ–µ—Ç –≤ {improvement:.2f}√ó –≤—ã—à–µ variance (p<0.05)\")\n",
    "                print(f\"   ‚Üí –ü—Ä–æ—Å—Ç–æ –±–æ–ª—å—à–µ dimensions –ü–û–ú–û–ì–ê–ï–¢\")\n",
    "            else:\n",
    "                print(f\"   ‚úó –ù–ï –ó–ù–ê–ß–ò–ú–û\")\n",
    "\n",
    "        # 2. Dense_128 vs V4_128 (–ß–ï–°–¢–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï!)\n",
    "        if 'Dense_ReLU_128' in self.summary and 'V4_KSparse_128' in self.summary:\n",
    "            print(\"\\n2. Dense_128 vs V4_128 (üéØ –ß–ï–°–¢–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï):\")\n",
    "            print(\"   \" + \"-\"*60)\n",
    "\n",
    "            vars_dense = self.summary['Dense_ReLU_128']['variances']\n",
    "            vars_v4 = self.summary['V4_KSparse_128']['variances']\n",
    "\n",
    "            dead_dense = self.summary['Dense_ReLU_128']['dead_pct_mean']\n",
    "            dead_v4 = self.summary['V4_KSparse_128']['dead_pct_mean']\n",
    "\n",
    "            t_stat, p_value = stats.ttest_ind(vars_dense, vars_v4)\n",
    "\n",
    "            mean_dense = np.mean(vars_dense)\n",
    "            mean_v4 = np.mean(vars_v4)\n",
    "            improvement = mean_v4 / mean_dense\n",
    "\n",
    "            print(f\"   Variance: {mean_dense:.4f} ‚Üí {mean_v4:.4f}\")\n",
    "            print(f\"   Dead neurons: {dead_dense:.1%} ‚Üí {dead_v4:.1%}\")\n",
    "            print(f\"   t-statistic: {t_stat:.4f}\")\n",
    "            print(f\"   p-value: {p_value:.6f}\")\n",
    "\n",
    "            if p_value < 0.05:\n",
    "                print(f\"   ‚úì –ó–ù–ê–ß–ò–ú–û: V4 –∏–º–µ–µ—Ç –≤ {improvement:.2f}√ó –≤—ã—à–µ variance (p<0.05)\")\n",
    "                print(f\"   ‚úì –ó–ù–ê–ß–ò–ú–û: V4 –∏–º–µ–µ—Ç {dead_v4:.1%} dead vs {dead_dense:.1%}\")\n",
    "                print(f\"\\n   üéØ –≠–¢–û –í–ê–® –ì–õ–ê–í–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢ –î–õ–Ø –°–¢–ê–¢–¨–ò!\")\n",
    "                print(f\"   ‚Üí K-Sparse + Chaos activation –†–ê–ë–û–¢–ê–ï–¢ –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π capacity\")\n",
    "            else:\n",
    "                print(f\"   ‚úó –ù–ï –ó–ù–ê–ß–ò–ú–û\")\n",
    "\n",
    "            # Test that improvement is significant\n",
    "            self.assertLess(p_value, 0.05, \"V4 should significantly outperform Dense_128\")\n",
    "            self.assertGreater(improvement, 1.5, \"V4 should have at least 1.5√ó higher variance\")\n",
    "\n",
    "    def test_03_create_fair_comparison_plots(self):\n",
    "        \"\"\"Create detailed comparison visualization\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Creating Fair Comparison Plots\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "        arch_names = list(self.results.keys())\n",
    "        x = np.arange(len(arch_names))\n",
    "\n",
    "        # Colors\n",
    "        colors = {\n",
    "            'Dense_ReLU_64': 'lightcoral',\n",
    "            'Dense_ReLU_128': 'skyblue',\n",
    "            'V4_KSparse_128': 'lightgreen'\n",
    "        }\n",
    "        color_list = [colors[name] for name in arch_names]\n",
    "\n",
    "        # Collect data\n",
    "        variances_mean = []\n",
    "        variances_std = []\n",
    "        dead_pct_mean = []\n",
    "        dead_pct_std = []\n",
    "        loss_mean = []\n",
    "        loss_std = []\n",
    "\n",
    "        for arch in arch_names:\n",
    "            vars_list = [r['variance'] for r in self.results[arch]]\n",
    "            dead_list = [r['dead_percentage'] * 100 for r in self.results[arch]]\n",
    "            loss_list = [r['val_loss'] for r in self.results[arch]]\n",
    "\n",
    "            variances_mean.append(np.mean(vars_list))\n",
    "            variances_std.append(np.std(vars_list))\n",
    "            dead_pct_mean.append(np.mean(dead_list))\n",
    "            dead_pct_std.append(np.std(dead_list))\n",
    "            loss_mean.append(np.mean(loss_list))\n",
    "            loss_std.append(np.std(loss_list))\n",
    "\n",
    "        # Plot 1: Variance (–≥–ª–∞–≤–Ω—ã–π!)\n",
    "        axes[0, 0].bar(x, variances_mean, yerr=variances_std, capsize=5,\n",
    "                       alpha=0.7, color=color_list, edgecolor='black', linewidth=1.5)\n",
    "        axes[0, 0].set_xticks(x)\n",
    "        axes[0, 0].set_xticklabels(arch_names, rotation=15, ha='right', fontsize=10)\n",
    "        axes[0, 0].set_ylabel('Mean Variance', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].set_title('Variance Comparison\\n(FAIR: Same capacity for 128-dim models)',\n",
    "                             fontweight='bold', fontsize=13)\n",
    "        axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "        if len(arch_names) >= 3:\n",
    "            # –°—Ç—Ä–µ–ª–∫–∞ –º–µ–∂–¥—É Dense_128 –∏ V4_128\n",
    "            axes[0, 0].annotate('', xy=(2, variances_mean[2]), xytext=(1, variances_mean[1]),\n",
    "                                arrowprops=dict(arrowstyle='<->', color='red', lw=2.5))\n",
    "\n",
    "            improvement = variances_mean[2] / variances_mean[1]\n",
    "            mid_y = (variances_mean[1] + variances_mean[2]) / 2\n",
    "\n",
    "            axes[0, 0].text(1.5, mid_y, f'{improvement:.2f}√ó\\nFAIR',\n",
    "                           ha='center', va='center', fontsize=12, color='red', fontweight='bold',\n",
    "                           bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow',\n",
    "                                     edgecolor='red', linewidth=2, alpha=0.8))\n",
    "\n",
    "            # Unfair —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–µ—Ä—ã–º\n",
    "            axes[0, 0].annotate('', xy=(2, variances_mean[2]), xytext=(0, variances_mean[0]),\n",
    "                                arrowprops=dict(arrowstyle='<->', color='gray', lw=1,\n",
    "                                                linestyle='--', alpha=0.5))\n",
    "\n",
    "            unfair_improvement = variances_mean[2] / variances_mean[0]\n",
    "            axes[0, 0].text(1.0, (variances_mean[0] + variances_mean[2])/2 + 0.05,\n",
    "                           f'{unfair_improvement:.2f}√ó (unfair)',\n",
    "                           ha='center', fontsize=9, color='gray', style='italic', alpha=0.7)\n",
    "\n",
    "        # Plot 2: Dead Neurons %\n",
    "        axes[0, 1].bar(x, dead_pct_mean, yerr=dead_pct_std, capsize=5,\n",
    "                       alpha=0.7, color=color_list, edgecolor='black', linewidth=1.5)\n",
    "        axes[0, 1].set_xticks(x)\n",
    "        axes[0, 1].set_xticklabels(arch_names, rotation=15, ha='right', fontsize=10)\n",
    "        axes[0, 1].set_ylabel('Dead Neurons (%)', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].set_title('Dead Neurons Comparison\\n(Lower is better)',\n",
    "                             fontweight='bold', fontsize=13)\n",
    "        axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "        axes[0, 1].set_ylim(0, max(dead_pct_mean) * 1.2 if max(dead_pct_mean) > 0 else 10)\n",
    "\n",
    "        # Plot 3: Loss\n",
    "        axes[1, 0].bar(x, loss_mean, yerr=loss_std, capsize=5,\n",
    "                       alpha=0.7, color=color_list, edgecolor='black', linewidth=1.5)\n",
    "        axes[1, 0].set_xticks(x)\n",
    "        axes[1, 0].set_xticklabels(arch_names, rotation=15, ha='right', fontsize=10)\n",
    "        axes[1, 0].set_ylabel('Validation Loss (MSE)', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].set_title('Reconstruction Quality\\n(Lower is better)',\n",
    "                             fontweight='bold', fontsize=13)\n",
    "        axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # Plot 4: Summary table\n",
    "        axes[1, 1].axis('off')\n",
    "\n",
    "        table_data = []\n",
    "        for idx, arch in enumerate(arch_names):\n",
    "            latent_dim = 64 if '64' in arch else 128\n",
    "            table_data.append([\n",
    "                arch.replace('_', '\\n'),\n",
    "                f\"{latent_dim}\",\n",
    "                f\"{variances_mean[idx]:.3f}\",\n",
    "                f\"{dead_pct_mean[idx]:.1f}%\"\n",
    "            ])\n",
    "\n",
    "        table = axes[1, 1].table(cellText=table_data,\n",
    "                                 colLabels=['Architecture', 'Dims', 'Variance', 'Dead %'],\n",
    "                                 cellLoc='center',\n",
    "                                 loc='center',\n",
    "                                 bbox=[0, 0.3, 1, 0.6])\n",
    "\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 2)\n",
    "\n",
    "        # Highlight —á–µ—Å—Ç–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
    "        for i, arch in enumerate(arch_names):\n",
    "            if '128' in arch:\n",
    "                for j in range(4):\n",
    "                    table[(i+1, j)].set_facecolor('lightyellow')\n",
    "                    table[(i+1, j)].set_edgecolor('orange')\n",
    "                    table[(i+1, j)].set_linewidth(2)\n",
    "\n",
    "        axes[1, 1].text(0.5, 0.15, '‚ö†Ô∏è Yellow rows: Fair comparison (same capacity)',\n",
    "                       ha='center', fontsize=10, style='italic',\n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "        axes[1, 1].set_title('Summary Table', fontweight='bold', fontsize=13)\n",
    "\n",
    "        plt.suptitle(f'Fair Baseline Comparison (N={self.num_runs} runs)',\n",
    "                     fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig('fair_baseline_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: fair_baseline_comparison.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def test_04_create_latex_table(self):\n",
    "        \"\"\"Create LaTeX table for paper\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"LATEX TABLE FOR PAPER (Fair Comparison)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        print(\"\\n\\\\begin{table}[h]\")\n",
    "        print(\"\\\\centering\")\n",
    "        print(\"\\\\caption{Fair Baseline Comparison on Logistic Map Dataset}\")\n",
    "        print(\"\\\\label{tab:fair_comparison}\")\n",
    "        print(\"\\\\begin{tabular}{lcccc}\")\n",
    "        print(\"\\\\hline\")\n",
    "        print(\"Architecture & Latent Dim & Variance & Dead Neurons & Val Loss \\\\\\\\\")\n",
    "        print(\"\\\\hline\")\n",
    "\n",
    "        for arch_name, runs in self.results.items():\n",
    "            variances = [r['variance'] for r in runs]\n",
    "            dead_pcts = [r['dead_percentage'] for r in runs]\n",
    "            losses = [r['val_loss'] for r in runs]\n",
    "\n",
    "            var_mean = np.mean(variances)\n",
    "            var_std = np.std(variances)\n",
    "            dead_pct = np.mean(dead_pcts)\n",
    "            loss_mean = np.mean(losses)\n",
    "            loss_std = np.std(losses)\n",
    "\n",
    "            latent_dim = 64 if '64' in arch_name else 128\n",
    "\n",
    "            # Format name\n",
    "            if 'Dense_ReLU_64' in arch_name:\n",
    "                name = \"Dense ReLU\"\n",
    "            elif 'Dense_ReLU_128' in arch_name:\n",
    "                name = \"Dense ReLU (fair)\"\n",
    "            else:\n",
    "                name = \"\\\\textbf{K-Sparse Chaos (V4)}\"\n",
    "\n",
    "            print(f\"{name} & {latent_dim} & \"\n",
    "                  f\"${var_mean:.3f} \\\\pm {var_std:.3f}$ & \"\n",
    "                  f\"{dead_pct:.1%} & \"\n",
    "                  f\"${loss_mean:.4f} \\\\pm {loss_std:.4f}$ \\\\\\\\\")\n",
    "\n",
    "        print(\"\\\\hline\")\n",
    "\n",
    "        # Add improvement\n",
    "        if 'Dense_ReLU_128' in self.results and 'V4_KSparse_128' in self.results:\n",
    "            vars_dense = [r['variance'] for r in self.results['Dense_ReLU_128']]\n",
    "            vars_v4 = [r['variance'] for r in self.results['V4_KSparse_128']]\n",
    "\n",
    "            dead_dense = np.mean([r['dead_percentage'] for r in self.results['Dense_ReLU_128']])\n",
    "            dead_v4 = np.mean([r['dead_percentage'] for r in self.results['V4_KSparse_128']])\n",
    "\n",
    "            improvement = np.mean(vars_v4) / np.mean(vars_dense)\n",
    "\n",
    "            print(\"\\\\hline\")\n",
    "            print(f\"\\\\multicolumn{{5}}{{l}}{{\\\\textit{{Fair improvement (V4 vs Dense-128): \"\n",
    "                  f\"{improvement:.2f}√ó variance, {dead_v4:.0%} vs {dead_dense:.0%} dead neurons}}}} \\\\\\\\\")\n",
    "\n",
    "        print(\"\\\\end{tabular}\")\n",
    "        print(\"\\\\end{table}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìã –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç—É —Ç–∞–±–ª–∏—Ü—É –≤ –≤–∞—à—É —Å—Ç–∞—Ç—å—é!\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "############################################\n",
    "# TEST CLASS 3: MULTIPLE RUNS (—Å—Ç–∞—Ä—ã–π)\n",
    "############################################\n",
    "\n",
    "class TestMultipleRuns(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    CRITICAL EXPERIMENT #3: Multiple Runs for Statistical Significance\n",
    "    (–°—Ç–∞—Ä—ã–π —Ç–µ—Å—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ TestFairBaselineComparison –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ)\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MULTIPLE RUNS FOR STATISTICAL SIGNIFICANCE (Legacy)\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ TestFairBaselineComparison –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\")\n",
    "\n",
    "        cls.num_runs = 5\n",
    "        cls.architectures = {\n",
    "            'Dense_ReLU': lambda: build_dense_relu_ae(latent_dim=64),\n",
    "            'KSparse_Chaos': lambda: build_ksparse_chaos_ae(latent_dim=128, k_active=32)\n",
    "        }\n",
    "\n",
    "        cls.results = {name: [] for name in cls.architectures.keys()}\n",
    "\n",
    "        # Generate data once\n",
    "        cls.train_images = generate_logistic_dataset(2000, fixed_initial=False)\n",
    "        cls.test_images = generate_logistic_dataset(500, fixed_initial=False)\n",
    "\n",
    "        # Run experiments\n",
    "        for arch_name, builder in cls.architectures.items():\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Architecture: {arch_name}\")\n",
    "            print(f\"{'='*70}\")\n",
    "\n",
    "            for run in range(cls.num_runs):\n",
    "                print(f\"\\n[Run {run+1}/{cls.num_runs}]\")\n",
    "\n",
    "                # Set seeds\n",
    "                np.random.seed(run)\n",
    "                tf.random.set_seed(run)\n",
    "\n",
    "                # Build and train\n",
    "                ae, enc = builder()\n",
    "                history = ae.fit(\n",
    "                    cls.train_images, cls.train_images,\n",
    "                    epochs=10,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Analyze\n",
    "                stats = analyze_latent_statistics(enc, cls.test_images)\n",
    "\n",
    "                cls.results[arch_name].append({\n",
    "                    'run': run,\n",
    "                    'stats': stats,\n",
    "                    'val_loss': history.history['val_loss'][-1]\n",
    "                })\n",
    "\n",
    "                print(f\"  Variance: {stats['mean_variance']:.6f}\")\n",
    "                print(f\"  Dead neurons: {stats['dead_neurons']}/{stats['total_neurons']}\")\n",
    "                print(f\"  Val loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "    def test_01_compute_statistics(self):\n",
    "        \"\"\"Compute mean, std, confidence intervals\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Statistical Summary\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        for arch_name, runs in self.results.items():\n",
    "            print(f\"\\n{arch_name}:\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "            # Extract metrics\n",
    "            variances = [r['stats']['mean_variance'] for r in runs]\n",
    "            dead_counts = [r['stats']['dead_neurons'] for r in runs]\n",
    "            losses = [r['val_loss'] for r in runs]\n",
    "\n",
    "            # Compute statistics\n",
    "            var_mean = np.mean(variances)\n",
    "            var_std = np.std(variances)\n",
    "            var_ci = 1.96 * var_std / np.sqrt(len(variances))\n",
    "\n",
    "            dead_mean = np.mean(dead_counts)\n",
    "            dead_std = np.std(dead_counts)\n",
    "\n",
    "            loss_mean = np.mean(losses)\n",
    "            loss_std = np.std(losses)\n",
    "\n",
    "            print(f\"Variance:     {var_mean:.6f} ¬± {var_std:.6f} (CI: ¬±{var_ci:.6f})\")\n",
    "            print(f\"Dead neurons: {dead_mean:.1f} ¬± {dead_std:.1f}\")\n",
    "            print(f\"Val loss:     {loss_mean:.6f} ¬± {loss_std:.6f}\")\n",
    "\n",
    "            if not hasattr(self, 'summary'):\n",
    "                self.summary = {}\n",
    "            self.summary[arch_name] = {\n",
    "                'variance_mean': var_mean,\n",
    "                'variance_std': var_std,\n",
    "                'variance_ci': var_ci,\n",
    "                'dead_mean': dead_mean,\n",
    "                'loss_mean': loss_mean\n",
    "            }\n",
    "\n",
    "    def test_02_statistical_comparison(self):\n",
    "        \"\"\"Compare architectures with t-test\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Statistical Comparison (t-test)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        arch_names = list(self.results.keys())\n",
    "\n",
    "        if len(arch_names) >= 2:\n",
    "            arch1, arch2 = arch_names[0], arch_names[1]\n",
    "\n",
    "            vars1 = [r['stats']['mean_variance'] for r in self.results[arch1]]\n",
    "            vars2 = [r['stats']['mean_variance'] for r in self.results[arch2]]\n",
    "\n",
    "            t_stat, p_value = stats.ttest_ind(vars1, vars2)\n",
    "\n",
    "            print(f\"\\nComparing {arch1} vs {arch2}:\")\n",
    "            print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "            print(f\"  p-value: {p_value:.6f}\")\n",
    "\n",
    "            if p_value < 0.05:\n",
    "                print(f\"  ‚úì SIGNIFICANT difference (p < 0.05)\")\n",
    "                if np.mean(vars2) > np.mean(vars1):\n",
    "                    print(f\"  ‚Üí {arch2} has significantly higher variance\")\n",
    "            else:\n",
    "                print(f\"  ‚úó No significant difference (p >= 0.05)\")\n",
    "\n",
    "    def test_03_create_errorbar_plot(self):\n",
    "        \"\"\"Create plot with error bars\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Creating Error Bar Plots\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        arch_names = list(self.results.keys())\n",
    "        x = np.arange(len(arch_names))\n",
    "\n",
    "        # Collect data\n",
    "        variances_mean = []\n",
    "        variances_std = []\n",
    "        dead_mean = []\n",
    "        dead_std = []\n",
    "        loss_mean = []\n",
    "        loss_std = []\n",
    "\n",
    "        for arch in arch_names:\n",
    "            vars_list = [r['stats']['mean_variance'] for r in self.results[arch]]\n",
    "            dead_list = [r['stats']['dead_neurons'] for r in self.results[arch]]\n",
    "            loss_list = [r['val_loss'] for r in self.results[arch]]\n",
    "\n",
    "            variances_mean.append(np.mean(vars_list))\n",
    "            variances_std.append(np.std(vars_list))\n",
    "            dead_mean.append(np.mean(dead_list))\n",
    "            dead_std.append(np.std(dead_list))\n",
    "            loss_mean.append(np.mean(loss_list))\n",
    "            loss_std.append(np.std(loss_list))\n",
    "\n",
    "        # Plot 1: Variance\n",
    "        axes[0].bar(x, variances_mean, yerr=variances_std, capsize=5, alpha=0.7)\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(arch_names, rotation=45, ha='right')\n",
    "        axes[0].set_ylabel('Mean Variance')\n",
    "        axes[0].set_title('Variance Comparison\\n(with std error bars)', fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # Plot 2: Dead Neurons\n",
    "        axes[1].bar(x, dead_mean, yerr=dead_std, capsize=5, alpha=0.7, color='orange')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(arch_names, rotation=45, ha='right')\n",
    "        axes[1].set_ylabel('Dead Neurons')\n",
    "        axes[1].set_title('Dead Neurons Comparison\\n(with std error bars)', fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # Plot 3: Loss\n",
    "        axes[2].bar(x, loss_mean, yerr=loss_std, capsize=5, alpha=0.7, color='green')\n",
    "        axes[2].set_xticks(x)\n",
    "        axes[2].set_xticklabels(arch_names, rotation=45, ha='right')\n",
    "        axes[2].set_ylabel('Validation Loss (MSE)')\n",
    "        axes[2].set_title('Reconstruction Loss Comparison\\n(with std error bars)', fontweight='bold')\n",
    "        axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        plt.suptitle(f'Multiple Runs Comparison (N={self.num_runs} runs)',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('multiple_runs_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: multiple_runs_comparison.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "class TestHenonGeneralization(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    CRITICAL EXPERIMENT #4: Henon Map Dataset\n",
    "    Test if results generalize to different chaotic system\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"HENON MAP GENERALIZATION TEST\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Generate both datasets\n",
    "        cls.logistic_train = generate_logistic_dataset(2000, fixed_initial=False)\n",
    "        cls.henon_train = generate_henon_dataset(2000)\n",
    "\n",
    "        cls.logistic_test = generate_logistic_dataset(500, fixed_initial=False)\n",
    "        cls.henon_test = generate_henon_dataset(500)\n",
    "\n",
    "        print(f\"Logistic train: {cls.logistic_train.shape}\")\n",
    "        print(f\"Henon train: {cls.henon_train.shape}\")\n",
    "\n",
    "        cls.results = {}\n",
    "\n",
    "        # Train on both datasets\n",
    "        for dataset_name, train_data, test_data in [\n",
    "            ('Logistic', cls.logistic_train, cls.logistic_test),\n",
    "            ('Henon', cls.henon_train, cls.henon_test)\n",
    "        ]:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Training K-Sparse Chaos on {dataset_name} Map\")\n",
    "            print(f\"{'='*70}\")\n",
    "\n",
    "            ae, enc = build_ksparse_chaos_ae(latent_dim=128, k_active=32)\n",
    "\n",
    "            history = ae.fit(\n",
    "                train_data, train_data,\n",
    "                epochs=10,\n",
    "                batch_size=64,\n",
    "                validation_split=0.1,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            stats = analyze_latent_statistics(enc, test_data)\n",
    "\n",
    "            cls.results[dataset_name] = {\n",
    "                'autoencoder': ae,\n",
    "                'encoder': enc,\n",
    "                'stats': stats,\n",
    "                'val_loss': history.history['val_loss'][-1]\n",
    "            }\n",
    "\n",
    "            print(f\"\\nResults:\")\n",
    "            print(f\"  Variance: {stats['mean_variance']:.6f}\")\n",
    "            print(f\"  Dead neurons: {stats['dead_neurons']}/{stats['total_neurons']}\")\n",
    "            print(f\"  Val loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "    def test_01_compare_datasets(self):\n",
    "        \"\"\"Compare results across datasets\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Logistic vs Henon Comparison\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        print(f\"\\n{'Metric':<25} {'Logistic':<15} {'Henon':<15} {'Ratio'}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        log_var = self.results['Logistic']['stats']['mean_variance']\n",
    "        hen_var = self.results['Henon']['stats']['mean_variance']\n",
    "        print(f\"{'Variance':<25} {log_var:<15.6f} {hen_var:<15.6f} {hen_var/log_var:.2f}√ó\")\n",
    "\n",
    "        log_dead = self.results['Logistic']['stats']['dead_neurons']\n",
    "        hen_dead = self.results['Henon']['stats']['dead_neurons']\n",
    "        print(f\"{'Dead Neurons':<25} {log_dead:<15} {hen_dead:<15} -\")\n",
    "\n",
    "        log_loss = self.results['Logistic']['val_loss']\n",
    "        hen_loss = self.results['Henon']['val_loss']\n",
    "        print(f\"{'Val Loss':<25} {log_loss:<15.6f} {hen_loss:<15.6f} {hen_loss/log_loss:.2f}√ó\")\n",
    "\n",
    "        # Check consistency\n",
    "        self.assertEqual(log_dead, 0, \"Logistic: should have 0 dead neurons\")\n",
    "        self.assertEqual(hen_dead, 0, \"Henon: should have 0 dead neurons\")\n",
    "\n",
    "        # Variance should be in same ballpark (within 3√ó)\n",
    "        variance_ratio = max(log_var, hen_var) / min(log_var, hen_var)\n",
    "        self.assertLess(variance_ratio, 3.0,\n",
    "                       f\"Variance too different between datasets: {variance_ratio:.2f}√ó\")\n",
    "\n",
    "    def test_02_visualize_latent_spaces(self):\n",
    "        \"\"\"Visualize and compare latent spaces\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Visualizing Latent Spaces\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "        for idx, (name, dataset) in enumerate([\n",
    "            ('Logistic', self.logistic_test),\n",
    "            ('Henon', self.henon_test)\n",
    "        ]):\n",
    "            enc = self.results[name]['encoder']\n",
    "            latents = enc.predict(dataset[:100], verbose=0)\n",
    "\n",
    "            # 2D projection\n",
    "            axes[0, idx].scatter(latents[:, 0], latents[:, 1],\n",
    "                               alpha=0.6, s=30, edgecolors='black', linewidths=0.5)\n",
    "            axes[0, idx].set_xlabel('Dim 0')\n",
    "            axes[0, idx].set_ylabel('Dim 1')\n",
    "            axes[0, idx].set_title(f'{name} Map\\nLatent Space (first 2 dims)',\n",
    "                                  fontweight='bold')\n",
    "            axes[0, idx].grid(True, alpha=0.3)\n",
    "\n",
    "            # Variance distribution\n",
    "            variance = np.var(latents, axis=0)\n",
    "            axes[1, idx].hist(np.log10(variance + 1e-10), bins=20, alpha=0.7,\n",
    "                            edgecolor='black')\n",
    "            axes[1, idx].set_xlabel('Log10(Variance)')\n",
    "            axes[1, idx].set_ylabel('Frequency')\n",
    "            axes[1, idx].set_title(f'{name} Map\\nVariance Distribution',\n",
    "                                  fontweight='bold')\n",
    "            axes[1, idx].axvline(x=np.log10(self.results[name]['stats']['mean_variance']),\n",
    "                               color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "            axes[1, idx].legend()\n",
    "            axes[1, idx].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.suptitle('Generalization Test: Logistic vs Henon Map',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('henon_generalization.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: henon_generalization.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "class TestDeadNeuronTrajectory(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    EXPERIMENT #5: Track how neurons die during training\n",
    "    \"\"\"\n",
    "\n",
    "    def test_01_track_broken_ae_death(self):\n",
    "        \"\"\"Track neuron death in broken L1 architecture\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Tracking Neuron Death (Broken L1 AE)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        self.skipTest(\"Requires build_sparse_ae_broken function\")\n",
    "\n",
    "    def test_02_track_chaos_ae_stability(self):\n",
    "        \"\"\"Verify neurons stay alive in Chaos AE\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST: Neuron Stability (K-Sparse Chaos AE)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        images = generate_logistic_dataset(1000, fixed_initial=False)\n",
    "\n",
    "        ae, enc = build_ksparse_chaos_ae(latent_dim=128, k_active=32)\n",
    "\n",
    "        trajectory = track_dead_neurons_over_time(ae, enc, images, epochs=30)\n",
    "\n",
    "        # Plot trajectory\n",
    "        epochs = [t['epoch'] for t in trajectory]\n",
    "        dead = [t['dead_neurons'] for t in trajectory]\n",
    "        variance = [t['mean_variance'] for t in trajectory]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        axes[0].plot(epochs, dead, 'o-', linewidth=2)\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Dead Neurons')\n",
    "        axes[0].set_title('Dead Neurons Over Time', fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1].plot(epochs, variance, 'o-', linewidth=2, color='green')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Mean Variance')\n",
    "        axes[1].set_title('Variance Over Time', fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.suptitle('K-Sparse Chaos AE: Training Stability',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_stability.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: training_stability.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Assertions\n",
    "        final_dead = trajectory[-1]['dead_neurons']\n",
    "        self.assertEqual(final_dead, 0, \"Should maintain 0 dead neurons\")\n",
    "\n",
    "        final_variance = trajectory[-1]['mean_variance']\n",
    "        initial_variance = trajectory[5]['mean_variance']\n",
    "        self.assertGreater(final_variance, initial_variance * 0.8,\n",
    "                          \"Variance should not collapse during training\")\n",
    "\n",
    "\n",
    "def run_all_critical_experiments():\n",
    "    \"\"\"Run all critical experiments for paper\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING ALL CRITICAL EXPERIMENTS (WITH FAIR BASELINE)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Create test suite\n",
    "    suite = unittest.TestSuite()\n",
    "\n",
    "    # Add test classes in order of priority\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestKSparseAblation))\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestFairBaselineComparison))  # ‚Üê –ù–û–í–û–ï!\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMultipleRuns))\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestHenonGeneralization))\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestDeadNeuronTrajectory))\n",
    "\n",
    "    # Run with detailed output\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total tests run: {result.testsRun}\")\n",
    "    print(f\"Successes: {result.testsRun - len(result.failures) - len(result.errors)}\")\n",
    "    print(f\"Failures: {len(result.failures)}\")\n",
    "    print(f\"Errors: {len(result.errors)}\")\n",
    "\n",
    "    if result.wasSuccessful():\n",
    "        print(\"\\n‚úÖ ALL CRITICAL EXPERIMENTS PASSED!\")\n",
    "        print(\"\\nGenerated files:\")\n",
    "        print(\"  - k_sparse_ablation.png\")\n",
    "        print(\"  - fair_baseline_comparison.png\")\n",
    "        print(\"  - multiple_runs_comparison.png\")\n",
    "        print(\"  - henon_generalization.png\")\n",
    "        print(\"  - training_stability.png\")\n",
    "        print(\"\\nüéâ Ready for paper with FAIR BASELINE COMPARISON!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Some experiments failed. Review output above.\")\n",
    "\n",
    "    return result.wasSuccessful()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    success = run_all_critical_experiments()\n",
    "    exit(0 if success else 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
