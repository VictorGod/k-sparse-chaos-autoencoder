{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3881984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [1] Генерация данных для baseline Dense AE ===\n",
      "\n",
      "=== [2] Построение Dense Autoencoder (ReLU) ===\n",
      "Model: \"dense_autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " enc_dense_1 (Dense)         (None, 256)               200960    \n",
      "                                                                 \n",
      " enc_dense_2 (Dense)         (None, 128)               32896     \n",
      "                                                                 \n",
      " latent (Dense)              (None, 64)                8256      \n",
      "                                                                 \n",
      " dec_dense_1 (Dense)         (None, 128)               8320      \n",
      "                                                                 \n",
      " dec_dense_2 (Dense)         (None, 256)               33024     \n",
      "                                                                 \n",
      " dec_out (Dense)             (None, 784)               201488    \n",
      "                                                                 \n",
      " output (Reshape)            (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484944 (1.85 MB)\n",
      "Trainable params: 484944 (1.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "=== [3] Обучение Dense AE ===\n",
      "Epoch 1/10\n",
      "79/79 [==============================] - 3s 18ms/step - loss: 0.1157 - val_loss: 0.1154\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.1148 - val_loss: 0.1147\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.1135 - val_loss: 0.1138\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.1126 - val_loss: 0.1134\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.1118 - val_loss: 0.1130\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.1113 - val_loss: 0.1128\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.1108 - val_loss: 0.1126\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.1104 - val_loss: 0.1125\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.1101 - val_loss: 0.1124\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.1097 - val_loss: 0.1123\n",
      "\n",
      "Final train loss (MSE):     0.109715\n",
      "Final validation loss (MSE): 0.112337\n",
      "\n",
      "=== [4] Анализ латентного пространства ===\n",
      "\n",
      "===== Dense AE Latent Statistics =====\n",
      "Latent shape: (1000, 64)\n",
      "Mean variance per dimension: 8.697817e-02\n",
      "Dead neurons: 36/64 (56.2%)\n",
      "First 10 variances: [0.         0.19673632 0.         0.         0.21423066 0.19838166\n",
      " 0.28650263 0.22038315 0.2718361  0.        ]\n",
      "\n",
      "=== [5] Тест хаотического расхождения ===\n",
      "\n",
      "===== Dense AE Latent Chaos Divergence =====\n",
      "step 0: distance = 3.801953\n",
      "step 1: distance = 3.739186\n",
      "step 2: distance = 3.710583\n",
      "step 3: distance = 3.037107\n",
      "step 4: distance = 2.565706\n",
      "step 5: distance = 3.167713\n",
      "step 6: distance = 2.547187\n",
      "step 7: distance = 3.706703\n",
      "step 8: distance = 4.206253\n",
      "step 9: distance = 3.603457\n",
      "Final/initial ratio: 0.948\n",
      "\n",
      "=== SUMMARY FOR PAPER TABLE ===\n",
      "Mean latent variance: 8.697817e-02\n",
      "Dead neurons: 36/64\n",
      "Chaos distances: [3.8019533157348633, 3.7391860485076904, 3.710582733154297, 3.03710675239563, 2.565706491470337, 3.167712926864624, 2.5471866130828857, 3.706702947616577, 4.206252574920654, 3.603457450866699]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from math import log2\n",
    "\n",
    "def logistic_map(x, r=3.99):\n",
    "    return r * x * (1 - x)\n",
    "\n",
    "def generate_logistic_map_image(image_size=28, initial_value=0.4, r=3.99):\n",
    "    \"\"\"\n",
    "    Генерирует одно изображение size×size из логистического отображения.\n",
    "    Значения в [0, 1].\n",
    "    \"\"\"\n",
    "    iterations = image_size * image_size\n",
    "    x = initial_value\n",
    "    seq = []\n",
    "    for _ in range(iterations):\n",
    "        x = logistic_map(x, r)\n",
    "        seq.append(x)\n",
    "    img = np.array(seq).reshape((image_size, image_size))\n",
    "    return img\n",
    "\n",
    "def generate_logistic_map_images_dataset(num_images, image_size=28, r=3.99, fixed_initial=False):\n",
    "    \"\"\"\n",
    "    Датасет из num_images карт логистического отображения.\n",
    "    Если fixed_initial=False — каждая картинка со своим случайным x0.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for _ in range(num_images):\n",
    "        init_val = 0.4 if fixed_initial else np.random.rand()\n",
    "        img = generate_logistic_map_image(image_size=image_size, initial_value=init_val, r=r)\n",
    "        dataset.append(img)\n",
    "    dataset = np.array(dataset)\n",
    "    return dataset[..., np.newaxis].astype(\"float32\")\n",
    "\n",
    "\n",
    "def build_dense_autoencoder(image_size=(28, 28), latent_dim=64):\n",
    "    \"\"\"\n",
    "    ЧИСТЫЙ baseline:\n",
    "    - только Dense слои\n",
    "    - только ReLU\n",
    "    - НИКАКОЙ sparsity, L1, chaos, TopK и т.д.\n",
    "    \"\"\"\n",
    "    h, w = image_size\n",
    "    input_img = keras.Input(shape=(h, w, 1), name=\"input\")\n",
    "    x = layers.Flatten(name=\"flatten\")(input_img)\n",
    "\n",
    "    # Энкодер\n",
    "    x = layers.Dense(256, activation=\"relu\", name=\"enc_dense_1\")(x)\n",
    "    x = layers.Dense(128, activation=\"relu\", name=\"enc_dense_2\")(x)\n",
    "    latent = layers.Dense(latent_dim, activation=\"relu\", name=\"latent\")(x)\n",
    "\n",
    "    encoder = keras.Model(input_img, latent, name=\"dense_encoder\")\n",
    "\n",
    "    # Декодер\n",
    "    x = layers.Dense(128, activation=\"relu\", name=\"dec_dense_1\")(latent)\n",
    "    x = layers.Dense(256, activation=\"relu\", name=\"dec_dense_2\")(x)\n",
    "    x = layers.Dense(h * w, activation=\"sigmoid\", name=\"dec_out\")(x)\n",
    "    decoded = layers.Reshape((h, w, 1), name=\"output\")(x)\n",
    "\n",
    "    autoencoder = keras.Model(input_img, decoded, name=\"dense_autoencoder\")\n",
    "    autoencoder.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "\n",
    "def analyze_latent_statistics(encoder, images, zero_threshold=1e-6, verbose=True):\n",
    "    \"\"\"\n",
    "    Считает:\n",
    "      - дисперсию по измерениям\n",
    "      - среднюю дисперсию\n",
    "      - долю “мёртвых” нейронов (всегда ~0)\n",
    "    \"\"\"\n",
    "    latents = encoder.predict(images, verbose=0)  # (N, D)\n",
    "    latents = np.asarray(latents)\n",
    "    variance_per_dim = np.var(latents, axis=0)\n",
    "    mean_variance = float(np.mean(variance_per_dim))\n",
    "\n",
    "    # \"Мёртвый\" нейрон: |h_i| < eps почти для всех сэмплов\n",
    "    dead_mask = np.all(np.abs(latents) < zero_threshold, axis=0)\n",
    "    dead_neurons = int(np.sum(dead_mask))\n",
    "    total_neurons = latents.shape[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n===== Dense AE Latent Statistics =====\")\n",
    "        print(f\"Latent shape: {latents.shape}\")\n",
    "        print(f\"Mean variance per dimension: {mean_variance:.6e}\")\n",
    "        print(f\"Dead neurons: {dead_neurons}/{total_neurons} ({dead_neurons/total_neurons:.1%})\")\n",
    "        print(\"First 10 variances:\", variance_per_dim[:10])\n",
    "\n",
    "    stats = {\n",
    "        \"variance_per_dim\": variance_per_dim,\n",
    "        \"mean_variance\": mean_variance,\n",
    "        \"dead_neurons\": dead_neurons,\n",
    "        \"total_neurons\": total_neurons,\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "\n",
    "def test_latent_chaos_behavior_dense(encoder,\n",
    "                                     num_steps=10,\n",
    "                                     image_size=28,\n",
    "                                     r=3.99,\n",
    "                                     init=0.4,\n",
    "                                     delta=1e-5):\n",
    "    \"\"\"\n",
    "    Два близких начальных значения x0 и x0+δ, на каждом шаге генерируем картинку,\n",
    "    пропускаем через encoder и считаем ||z1_t - z2_t||.\n",
    "\n",
    "    Это прямой аналог твоего теста для стандартного AE, но для Dense ReLU baseline.\n",
    "    \"\"\"\n",
    "    chain1 = []\n",
    "    chain2 = []\n",
    "    x1 = init\n",
    "    x2 = init + delta\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        img1 = generate_logistic_map_image(image_size=image_size, initial_value=x1, r=r)\n",
    "        img2 = generate_logistic_map_image(image_size=image_size, initial_value=x2, r=r)\n",
    "        chain1.append(img1)\n",
    "        chain2.append(img2)\n",
    "        x1 = logistic_map(x1, r=r)\n",
    "        x2 = logistic_map(x2, r=r)\n",
    "\n",
    "    chain1 = np.array(chain1)[..., np.newaxis].astype(\"float32\")\n",
    "    chain2 = np.array(chain2)[..., np.newaxis].astype(\"float32\")\n",
    "\n",
    "    latent1 = encoder.predict(chain1, verbose=0)\n",
    "    latent2 = encoder.predict(chain2, verbose=0)\n",
    "\n",
    "    distances = [float(np.linalg.norm(latent1[i] - latent2[i])) for i in range(num_steps)]\n",
    "\n",
    "    print(\"\\n===== Dense AE Latent Chaos Divergence =====\")\n",
    "    for i, d in enumerate(distances):\n",
    "        print(f\"step {i}: distance = {d:.6f}\")\n",
    "    if distances[0] > 0:\n",
    "        ratio = distances[-1] / distances[0]\n",
    "        print(f\"Final/initial ratio: {ratio:.3f}\")\n",
    "    else:\n",
    "        print(\"Initial distance is 0 → ratio undefined.\")\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def run_dense_ae_baseline_experiment(\n",
    "    num_train=5000,\n",
    "    num_val=1000,\n",
    "    image_size=28,\n",
    "    latent_dim=64,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    r=3.99,\n",
    "):\n",
    "    \"\"\"\n",
    "    Полный эксперимент baseline Dense AE:\n",
    "      1) генерация train/val набора логистических карт\n",
    "      2) обучение Dense AE (ReLU)\n",
    "      3) оценка reconstruction MSE\n",
    "      4) анализ дисперсии латента\n",
    "      5) тест хаотического расхождения\n",
    "    \"\"\"\n",
    "    print(\"\\n=== [1] Генерация данных для baseline Dense AE ===\")\n",
    "    train_images = generate_logistic_map_images_dataset(num_train, image_size=image_size, r=r, fixed_initial=False)\n",
    "    val_images = generate_logistic_map_images_dataset(num_val, image_size=image_size, r=r, fixed_initial=False)\n",
    "\n",
    "    print(\"\\n=== [2] Построение Dense Autoencoder (ReLU) ===\")\n",
    "    autoencoder, encoder = build_dense_autoencoder(image_size=(image_size, image_size), latent_dim=latent_dim)\n",
    "    autoencoder.summary()\n",
    "\n",
    "    print(\"\\n=== [3] Обучение Dense AE ===\")\n",
    "    history = autoencoder.fit(\n",
    "        train_images,\n",
    "        train_images,\n",
    "        validation_data=(val_images, val_images),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    final_loss = history.history[\"loss\"][-1]\n",
    "    final_val_loss = history.history[\"val_loss\"][-1]\n",
    "    print(f\"\\nFinal train loss (MSE):     {final_loss:.6f}\")\n",
    "    print(f\"Final validation loss (MSE): {final_val_loss:.6f}\")\n",
    "\n",
    "    print(\"\\n=== [4] Анализ латентного пространства ===\")\n",
    "    stats = analyze_latent_statistics(encoder, val_images, verbose=True)\n",
    "\n",
    "    print(\"\\n=== [5] Тест хаотического расхождения ===\")\n",
    "    distances = test_latent_chaos_behavior_dense(encoder, num_steps=10, image_size=image_size, r=r)\n",
    "\n",
    "    results = {\n",
    "        \"autoencoder\": autoencoder,\n",
    "        \"encoder\": encoder,\n",
    "        \"history\": history.history,\n",
    "        \"latent_stats\": stats,\n",
    "        \"chaos_distances\": distances,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_dense_ae_baseline_experiment(\n",
    "        num_train=5000,\n",
    "        num_val=1000,\n",
    "        image_size=28,\n",
    "        latent_dim=64,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        r=3.99,\n",
    "    )\n",
    "\n",
    "    # Быстрый вывод ключевых чисел для включения в таблицу статьи\n",
    "    print(\"\\n=== SUMMARY FOR PAPER TABLE ===\")\n",
    "    print(f\"Mean latent variance: {results['latent_stats']['mean_variance']:.6e}\")\n",
    "    print(f\"Dead neurons: {results['latent_stats']['dead_neurons']}/{results['latent_stats']['total_neurons']}\")\n",
    "    print(f\"Chaos distances: {results['chaos_distances']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
